---
title: "Étude de cas - GLM et valeurs manquantes"
author: "Romane Lacoste-Badie, Margaux Touzé et Camille Loisel"
date: "02/03/2022"
header-includes:
  - \DeclareUnicodeCharacter{0301}{/}
  - \usepackage{float}
output:
  bookdown::pdf_document2: default
  number_section : true
  extra_dependecies : ["float"]
bibliography: mes_ref_biblio.bib
---

```{=tex}
\clearpage
\renewcommand{\contentsname}
\tableofcontents
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6.5, fig.height = 3.5, fig.pos="H", out.extra = "")
library(lmtest)
library(stats)
library(car)
library(MASS)
library(ResourceSelection)
library(ROCR)
library(dplyr)
library(knitr)
library(gridExtra)
library(ggplot2)
library(parallel)
library(stats)
library(mvtnorm)
library(visdat)
library(naniar)
library(UpSetR)
library(bookdown)
library(mice)
library(VIM)
library(lattice)
library(DescTools)
library(FactoMineR)
library(Hmisc)
library(gtsummary)
library(flextable)
library(base)
library(zoo)
library(bcv)
library(missForest)
library(locfit)
```

\clearpage

# Modèles linéaires généralisés (GLM) avec R

```{r, echo=F}
mites <- read.csv("mites.csv")
```

## Introduction

L'objectif de cette première partie est d'étudier le jeu de données sur les mites Oribatid mis à disposition par P. Legendre et D. Borcard et décrit dans leur article [@Borcard1994]. Tout particulièrement, nous essaierons de construire différents modèles linéaires simples puis généralisés pour décrire trois des variables de ce jeu de données.

Pour leur étude, Legendre et Borcard ont choisi une zone située à Saint-Hippolyte, Canada, qu'ils ont divisée en 70 "coeurs". Pour chaque coeur, ils ont récupéré des données environnementales et sur la faune. Le jeu de données que l'on étudie est donc constitué de 70 observations qui correspondent aux coeurs, et de 9 variables décrites ci-dessous.

```{r}
str(mites)
```

-   **Galumna** (entier) : nombre de mites *Galumna sp*.

-   **pa** (0 ou 1) : présence (1) ou absence (0) de mites *Galumna sp*.

-   **totalabund** (entier) : nombre total de mites

-   **prop** (réel entre 0 et 1) : proportion de mites *Galumna sp*.

-   **SubsDens** (réel) : densité du substrat en $g.L^{-1}$ de matière sèche non comprimée

-   **WatrCont** (réel) : contenu en eau du substrat en $g.L^{-1}$

-   **Substrate** (facteur) : type de substrat, facteur à 7 modalités

-   **Shrub** (facteur) : buissons, facteur à trois modalités

-   **Topo** (facteur) : microtopographie, facteur à 2 modalités

```{r, echo=F}
mites$pa <- as.factor(mites$pa)
mites$Substrate <- as.factor(mites$Substrate)
mites$Shrub <- as.factor(mites$Shrub)
mites$Topo <- as.factor(mites$Topo)
```

Nous allons maintenant représenter ces données afin de mieux les appréhender.\

\newpage

Commençons par les variables que l'on va chercher à expliquer par la suite : `Galumna`, `pa` et `prop.`

```{r, echo=F, fig.cap="Distribution de Galumna"}
ggplot(data = mites) + geom_bar(mapping = aes(Galumna), fill='darkred') +labs(y = "Nombre de coeurs", x = "Nombre de Galumna")
```

On remarque que plus de la moitié des coeurs étudiés ne contiennent pas de mites Galumna. La valeur maximale de Galumna contenues dans un coeur est 8. \newline

```{r, echo=F, fig.cap="Distribution de pa"}
ggplot(data = mites) + geom_bar(mapping = aes(pa), fill='darkred') +labs(y = "Nombre de coeurs", x = "Absence ou Présence de Galumna")
```

On voit ci-dessus plus précisément que 45 coeurs ne contiennent pas de Galumna et 25 coeurs en contiennent. \newline

```{r, echo=F, fig.cap="Distribution de prop"}
ggplot(data=mites) + geom_histogram(aes(prop), fill="darkred", bins = 30) +labs(y = "Nombre de coeurs", x = "Proportion de Galumna")
```

Naturellement, on remarque que 45 coeurs ont une proportion de Galumna de 0%. Pour les autres coeurs, la proportion varie entre 0 et 0.06%. \newline

Passons maintenant aux variables explicatives. \newline

```{r, echo=F, fig.cap="Distribution de SubsDens"}
ggplot(data=mites) + geom_histogram(aes(SubsDens), fill="darkred", binwidth = 5) +labs(y = "Nombre de coeurs", x = "SubsDens")
```

```{r, echo=F, fig.cap="Distribution de WatrCont"}
ggplot(data=mites) + geom_histogram(aes(WatrCont), fill="darkred", binwidth = 40) +labs(y = "Nombre de coeurs", x = "Watrcont")
```

Nous pouvons observer la distribution des variables quantitatives SubsDens et WatrCont. Cependant, ces variables étant déterministes, on ne fait pas d'hypothèse de normalité. Donc nous n'avons pas besoin de "vérifier" qu'elles suivent une loi normale, ni d'effectuer des transformations pour que cela soit le cas. \newline

```{r, echo=F, fig.cap="Distribution de Substrate"}
ggplot(data = mites) + geom_bar(mapping = aes(Substrate), fill="darkred")  +labs(y = "Nombre de coeurs", x = "Substrate")
```

Les deux types de substrat majoritaires sont "Interface" et "Sphagn1", qui sont chacun le substrat d'environ 25 coeurs sur les 70 totaux. \newline

```{r, echo=F, fig.cap="Distribution de Shrub"}
ggplot(data = mites) + geom_bar(mapping = aes(Shrub), fill="darkred")  +labs(y = "Nombre de coeurs", x = "Shrub")
```

La variable `Shrub` représente les buissons du coeur. On voit qu'un peu moins d'une vingtaine de coeurs ne contient pas de buisson. Puis environ 25 coeurs contiennent beaucoup de buissons et peu de buissons. \newline

```{r, echo=F, fig.cap="Distribution de Topo"}
ggplot(data = mites) + geom_bar(mapping = aes(Topo), fill="darkred")  +labs(y = "Nombre de coeurs", x = "Topo")
```

\newpage

Finalement, on voit qu'environ 45 coeurs ont une topographie de type "Blanket", les autres ont une topographie de type "Hummock".\
Ce premier coup d'oeil aux données nous a permis de remarquer quelque chose d'important : plus de la moitié des coeurs ne contiennent pas de Galumna. Cela siginifie, pour ceux-ci, que la proportion de Galumna est nulle, que la variable Galumna est égale à 0 et que la variable occurence de Galumna est également égale à 0. Il est important de garder cela à l'esprit pour la future construction de nos modèles.

\clearpage

## LM Gaussien

Dans un premier temps, nous allons essayer d'ajuster un modèle linéaire gaussien pour chaque variable réponse que l'on cherche à expliquer : `Galumna`, `prop`, `pa`. Mais un tel modèle doit vérifier plusieurs hypothèses. Le premier travail consiste donc à regarder si ces hypothèses sont vérifiées.

### Vérification des hypothèses

Rappelons les hypothèses du modèle linéaire gaussien :

-   sur la forme du modèle :\
    (H1) **linéarité** du modèle\

-   sur les erreurs $\epsilon_i$ qui sont :\
    (H2) **centrées** : $\mathbb{E}(\epsilon_i)=0, \forall i=1,..n$, cette condition est toujours vérifiée par les moindres carrés ordinaires (MCO), technique de résolution de la régression linéaire\
    (H3) **homoscédastiques** : $\mathbb{V}(\epsilon_i)=\sigma^2, \forall i$\
    (H4) **non-autocorrélées** : $cor(\epsilon_i, \epsilon_{i}^{'})=0, \forall i\neq i'$\
    (H5) **normales** : $\epsilon_i \sim N(0, \sigma^2)$\

-   sur les variables explicatives $X_1,...,X_p$ qui sont :\
    (H6) **non aléatoires** : la théorie se généralise facilement pour les variables aléatoires\
    (H7) **non multicolinéaires** : les variables $X_1,...,X_p$ sont linéairement indépendantes, ce qui garantit l'unicité de l'estimateur MCO.

Pour chaque modèle expliquant `Galumna`, `prop` et `pa`, nous regarderons si toutes ces hypothèses sont vérifiées, sauf la **2** et la **6** qui sont toujours vérifiées.

```{r, echo=F}
mites$pa <- as.numeric(as.character(mites$pa))
```

-   **(H1) Linéarité du modèle**

Nous allons observer la linéarité du modèle dans un premier temps par représentation graphique. Nous représenterons chaque variable à expliquer en fonction des variables `WatrCont` et `SubsDens` qui sont les deux seules variables quantitatives explicatives.

\newpage

**Variable réponse Galumna :**

```{r, echo=F, fig.cap="Modèles de régression sur la variable réponse Galumna"}
p1 <- ggplot(data=mites, aes(x=SubsDens, y=Galumna)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(Galumna~SubsDens)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(Galumna~SubsDens)"), values=c("darkred")) + theme(legend.position = "top")
p2 <- ggplot(data=mites, aes(x=WatrCont, y=Galumna)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(Galumna~WatrCont)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(Galumna~WatrCont)"), values=c("darkred")) + theme(legend.position = "top")
grid.arrange(p1,p2, nrow=1)
```

On ne voit pas un lien linéaire évident entre `Galumna` et `SubsDens`, ni entre `Galumna` et `WatrCont.` On voit que la quantité importante de points sur l'axe des abscisses (pour lesquels `Galumna`=0) semble beaucoup impacter la droite de régression.

**Variable réponse pa :**

```{r, echo=F, fig.cap="Modèles de régression sur la variable réponse pa"}
p1 <- ggplot(data=mites, aes(x=SubsDens, y=pa)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(pa~SubsDens)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(pa~SubsDens)"), values=c("darkred")) + theme(legend.position = "top")
p2 <- ggplot(data=mites, aes(x=WatrCont, y=pa)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(pa~WatrCont)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(pa~WatrCont)"), values=c("darkred")) + theme(legend.position = "top")
grid.arrange(p1,p2, nrow=1)
```

On ne voit pas non plus un lien linéaire entre la variable `pa` et les variables explicatives. De plus, comme c'est une variable binaire, il semblerait plus naturel de penser à une régression logistique qu'à une régression linéaire.

**Variable réponse prop :**

```{r, echo=F, fig.cap="Modèles de régression sur la variable réponse prop"}
p1 <- ggplot(data=mites, aes(x=SubsDens, y=prop)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(prop~SubsDens)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(prop~SubsDens)"), values=c("darkred")) + theme(legend.position = "top")
p2 <- ggplot(data=mites, aes(x=WatrCont, y=prop)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(prop~WatrCont)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(prop~WatrCont)"), values=c("darkred")) + theme(legend.position = "top")
grid.arrange(p1,p2, nrow=1)
```

On fait le même constat pour la variable `prop`, il ne semble pas y avoir un lien linéaire particulier et la présence de beaucoup de points sur l'axe des abscisses semble influencer fortement la droite de régression.

Pour pousser notre analyse plus loin, nous allons effectuer un test de Harvey-Collier sur les modèles qui expliquent nos variables réponses par les deux variables explicatives quantitatives (`SubsDens` et `WatrCont`). Le test de Harvey-Collier (fonction `lmtest:harvtest`) consiste à faire un test de Student sur les résidus récursifs. Si la réelle relation entre les variables n'est pas linéaire mais convexe ou concave, la moyenne des résidus récursifs devrait être significativement différente de 0.

```{r, echo=F}
kable(data.frame(c(harvtest(lm(Galumna~WatrCont+SubsDens, data=mites))$p.value,
                     harvtest(lm(pa~WatrCont+SubsDens, data=mites))$p.value,
                     harvtest(lm(prop~WatrCont+SubsDens, data=mites))$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
                  col.names = "p-value Harvey-Collier Test",
      caption="Harvey-Collier Test", position="H")
```

Au seuil de 5%, on peut conclure que :

-   la relation entre la variable `Galumna` et les variables `WatrCont` et `SubsDens` est linéaire

-   la relation entre la variable `pa` et les variables `WatrCont` et `SubsDens` n'est pas linéaire

-   la relation entre la variable `prop` et les variables `WatrCont` et `SubsDens` est linéaire

À ce stade de l'analyse, on peut déjà conclure qu'il ne sera pas possible d'ajuster un modèle linéaire sur le variable `pa` car l'hypothèse de linéarité n'est pas vérifiée.

-   **(H3) Erreurs** $\epsilon_i$ homoscédastiques

Pour la suite de la vérification des hypothèses, on ajuste un modèle complet sur chacune de nos variables réponses (i.e. un modèle qui fait intervenir toutes les variables explicatives à notre disposition).

```{r, echo=F}
lm.tot.g<-lm(Galumna~.-pa-prop-totalabund, data=mites)
lm.tot.pa<-lm(pa~.-Galumna-prop-totalabund, data=mites)
lm.tot.prop<-lm(prop~.-pa-Galumna-totalabund, data=mites)
```

```{r, echo=F, warning=F, fig.height=5, fig.cap="Variances des résidus des modèles de régression linéaire"}
par(mfrow=c(2,2))
plot(lm.tot.g, which = 3)
title(main="lm(Galumna~.)", line=1.4)
plot(lm.tot.pa, which = 3)
title(main="lm(pa~.)", line=1.4)
plot(lm.tot.prop, which = 3)
title(main="lm(prop~.)", line=1.4)
```

Le type de graphique représenté ci-dessus est utilisé pour évaluer l'homoscédasticité des résidus. Si leur variance est bien constante, alors la courbe rouge doit former une ligne à peu près horizontale. On voit que cette hypothèse est plutôt bien respectée pour les modèles expliquant les variables `Galumna` et `prop.` Par contre, l'hypothèse n'est manifestement pas vérifiée pour le modèle expliquant la variable `pa.`

La vérification de cette hypothèse peut également se faire par un test de Breusch-Pagan (fonction `lmtest:bptest`) pour lequel on a $H_0$ : le bruit est homoscédastique, i.e. $\sigma_i^2=\sigma^2$ pour tout $i$.

```{r, echo=F}
kable(data.frame(c(bptest(lm.tot.g)$p.value,
                     bptest(lm.tot.pa)$p.value,
                     bptest(lm.tot.prop)$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
                    col.names = "p-value Bruesch-Pagan Test",
                    caption="Breusch-Pagan Test", position="H")
```

Ce test confirme la conclusion basée sur les graphiques : on rejette l'hypothèse d'homoscédasticité des résidus pour le modèle expliquant `pa`, mais on l'accepte pour les deux autres modèles.

-   **(H4) Erreurs** $\epsilon_i$ non corrélées

Afin de tester la non-autocorrélation des résidus du modèle linéaire, nous allons effectuer un test de Durbin-Watson (fonction `lmtest:dwtest`). Ce test cherche à évaluer la significativité du coefficient $\rho$ dans la formule : $\epsilon_t = \rho \epsilon_{t-1} + u_t$ où $\epsilon_t$ est le résidu estimé du modèle et $u_t$ un bruit blanc avec le test de Wald. L'hypothèse nulle est donc $H_0 : \rho=0$, il y a non-autocorrélation des résidus.

```{r, echo=F}
kable(data.frame(c(dwtest(lm.tot.g)$p.value,
                     dwtest(lm.tot.pa)$p.value,
                     dwtest(lm.tot.prop)$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
      col.names = "p-value Durbin-Watson Test",
      caption="Durbin-Watson Test", position="H")
```

On observe une p-value inférieure à 0.05 pour les modèles expliquant `Galumna` et `prop.` On peut dire qu'au seuil de 5%, on considère que l'hypothèse de non-autocorrélation des erreurs n'est pas respectée pour ces deux modèles. Elle est cependant vérifiée pour le modèle linéaire expliquant `pa.`\
À ce stade de l'analyse, nous avons constaté que les hypothèses 1 et 3 ne sont pas vérifiées pour le modèle linéaire expliquant `pa`. L'hypothèse 4 n'est quant à elle pas vérifiée pour les modèles linéaires expliquant `Galumna` et `prop.` On pourrait donc s'arrêter et dire qu'il n'est pas possible d'ajuster un modèle linéaire sur ces trois variables. Par curiosité, nous allons continuer à regarder si les hypothèses suivantes sont vérifiées, bien que l'issue concernant les modèles linéaires reste la même.

-   **(H5) Normalité des erreurs** $\epsilon_i$

Dans le cadre d'un modèle linéaire, on suppose $\epsilon_i \sim N(0, \sigma^2)$. Une façon de vérifier cette hypothèse est de tracer les QQ-plot de nos modèles linéaires.

```{r, echo=F, warning =F, fig.height=5, fig.cap="QQ-plot des modèles de régression linéaire"}
par(mfrow=c(2,2))
plot(lm.tot.g, which = 2)
title(main="lm(Galumna~.)", line=1.4)
plot(lm.tot.pa, which = 2)
title(main="lm(pa~.)", line=1.4)
plot(lm.tot.prop, which = 2)
title(main="lm(prop~.)", line=1.4)
```

On voit que les points de nos différents modèles suivent à peu près tous la droite de normalité. Cependant, on remarque de nombreux points déviant de cette droite. Il est difficile de tirer une conclusion à partir de ces graphiques seuls. Il est également possible de réaliser un test de Shapiro-Wilk (fonction `stats:shapiro.test`). Il teste l'hypothèse nulle selon laquelle un échantillon $x_1,...,x_n$ est issu d'une population normalement distribuée. Ici, notre échantillon sera les $\epsilon_1,...,\epsilon_n$ du modèle linéaire considéré.

```{r, echo=F}
kable(data.frame(c(shapiro.test(lm.tot.g$residuals)$p.value,
                     shapiro.test(lm.tot.pa$residuals)$p.value,
                     shapiro.test(lm.tot.prop$residuals)$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
      col.names = "p-value Shapiro-Wilk Test",
      caption="Shapiro-Wilk Test", position="H")
```

On observe une très petite p-value pour les trois modèles linéaires. On considère que l'hypothèse de normalité des résidus n'est pas vérifiée pour nos trois modèles.

-   **(H7) Non-multicolinéarité des variables explicatives**

La dernière hypothèse qui doit être vérifiée lorsque l'on ajuste un modèle linéaire est la non-multicolinéarité des variables explicatives. Pour la vérifier, on peut faire de l'analyse bivariée, c'est-à-dire vérifier si les variables semblent être corrélées deux à deux, ou encore calculer les VIF.

**Corrélations SubsDens\~ :**

```{r, echo=F, fig.height=5, fig.cap="Corrélations avec la variable SubsDens"}
p1 <- ggplot(data=mites, aes(x = WatrCont, y=SubsDens)) + geom_point(color="darkred") + labs(title = "SubsDens ~ WatrCont") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p2 <- ggplot(data=mites, aes(Substrate, SubsDens)) + geom_boxplot(color="darkred") + labs(title = "SubsDens ~ Substrate") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold")) 

p3 <- ggplot(data=mites, aes(Shrub, SubsDens)) + geom_boxplot(color="darkred") + labs(title = "SubsDens ~ Shrub") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p4 <- ggplot(data=mites, aes(Topo, SubsDens)) + geom_boxplot(color="darkred") + labs(title = "SubsDens ~ Topo") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
grid.arrange(p1, p2, p3, p4, nrow=2)
```

Il ne semble pas y avoir de corrélation particulière entre les variables quantitatives `WatrCont` et `SubsDens`, les points sont répartis de manière assez aléatoire. Il ne semble pas y avoir de corrélation entre `SubsDens` et les variables `Shrub` et `Topo` non plus : la moyenne de `SubsDens` ne semble pas varier significativement selon le facteur `Shrub` ou `Topo.` Cependant, le type de substrat (variable `Substrate`) semble influencer significativement la densité du substrat (`SubsDens`), cela paraît assez logique.

\newpage

**Corrélations WatrCont\~ :**

```{r, echo=F, fig.height=5, fig.cap="Corrélations avec la variable WatrCont"}
p1 <- ggplot(data=mites, aes(Substrate, WatrCont)) + geom_boxplot(color="darkred") + labs(title = "WatrCont ~ Substrate") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p2 <- ggplot(data=mites, aes(Shrub, WatrCont)) + geom_boxplot(color="darkred") + labs(title = "WatrCont ~ Shrub") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p3 <- ggplot(data=mites, aes(Topo, WatrCont)) + geom_boxplot(color="darkred") + labs(title = "WatrCont ~ Topo") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
grid.arrange(p1, p2, p3, nrow=2)
```

Les variables `Substrate` et `Shrub` semblent avoir une influence significative sur la quantité d'eau (variable `WatrCont`). Cependant, il est difficile de conclure qu'il y a présence ou absence de corrélation entre `WatrCont` et `Topo.`

\newpage

**Corrélations Substrate\~ :**

```{r, echo=F, fig.height=5, fig.cap="Corrélations avec la variable Substrate"}
p1 <- ggplot(data=mites, aes(Substrate, fill=Shrub)) + geom_bar() +scale_fill_manual(values=c("#330000", "#990000", "#CC0000")) + labs(title = "Substrate ~ Shrub") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"), axis.text.x = element_text(angle=45, hjust=1))
p2 <- ggplot(data=mites, aes(Substrate, fill=Topo)) + geom_bar() +scale_fill_manual(values=c("#330000", "#990000", "#CC0000")) + labs(title = "Substrate ~ Topo") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"), axis.text.x = element_text(angle=45, hjust=1))
grid.arrange(p1, p2, nrow=1)
```

Du fait du peu de donées que nous avons à disposition, il est difficile de conclure à partir des graphiques.

\newpage

**Corrélations Shrub\~ :**

```{r, echo=F, fig.cap="Corrélations avec la variable Shrub"}
p1 <- ggplot(data=mites, aes(Shrub, fill=Topo)) + geom_bar() +scale_fill_manual(values=c("#330000", "#990000")) + labs(title = "Shrub ~ Topo") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
p1
```

Une nouvelle fois, au vu du faible nombre d'observations, il serait risqué de conclure uniquement à partir du graphique ci-dessus.

**Calcul des VIF :**

```{r, echo=F}
kable(data.frame(vif(lm.tot.g)[,1],vif(lm.tot.pa)[,1],vif(lm.tot.prop)[,1],
                    row.names=c("SubsDens", "WatrCont", "Substrate", "Shrub", "Topo")),
      col.names = c("lm(Galumna~.)","lm(pa~.)","lm(prop~.)"),
      caption="VIF", position="H")
```

Les VIF sont par définition toujours supérieurs à 1. On considère que le VIF d'une variable devient trop élevé lorsque sa valeur dépasse 5. Ce n'est pas le cas pour notre modèle. On peut donc conclure grâce aux VIF, et à notre analyse bivariée qu'il y a non-multicolinéarité entre les variables explicatives.

**Conclusion :** après analyse de toutes les hypothèses, on conclut qu'au moins une des hypothèses n'est pas vérifiée pour chacun de nos modèles linéaires. Il est donc impossible d'ajuster un modèle linéaire sur nos données pour expliquer les variables `Galumna`, `pa` et `prop.` Cependant, on peut essayer d'effectuer des transformations sur les données afin qu'elles vérifient les hypothèses.

\newpage

### Transformations sur les données

Transformer les données peut parfois régler les problèmes posés par les hypothèses du modèle linéaire gaussien, et notamment la non-normalité et l'hétéroscédasticité des résidus. Cependant, ces transformations présentent des inconvénients :

-   elles changent la variable réponse, ce qui peut compliquer l'interprétation

-   elles ne peuvent pas toujours améliorer la linéarité et l'homogénéité de la variance en même temps

-   les bornes de l'espace d'échantillonnage changent

Dans le cadre de notre étude, nous essaierons des transformations adaptées aux données en proportion pour notre variable `prop.` Ensuite, nous essaierons une transformation plus générale : celle de Box & Cox.

#### Transformation pour les données en proportion

 

Deux des principales transformations qui permettent de stabiliser la variance pour les données en proportion sont la transformation $logit$ et la transformation $arcsin$. Cependant, pour des données écologiques, la transformation $arcsin$ est souvent préférée car il est courant d'avoir des proportions à 0% ou 100%, ce qui pose problème pour effectuer une transformation $logit$. En effet, cela ferait donnerait lieu à des valeurs de $-\infty$ pour des proportions égales à 0. On rappelle que c'est notre cas : nous avions remarqué, lors de notre analyse préliminaire des données, que plusieurs observations avaient une valeur de 0 pour la variable prop. Il est donc impossible ici d'utiliser une transformation $logit$ sur les données.

**Transformation Arcsin :**

```{r, echo=F, fig.cap="Transformation Arcsin"}
asinTransform <- function(p) { asin(sqrt(p)) }
prop.asin <- asinTransform(mites$prop)
ggplot(data=data.frame(mites$prop, prop.asin),aes(x=mites$prop, y=prop.asin)) + geom_point() + geom_line(color="darkred")+ labs(x = "prop", y="Arcsin(prop)")
```

On voit ci-dessus que la transformation $arcsin$ semble finalement assez proche d'une transformation linéaire. Regardons ce que cela provoque sur les hypothèses du modèle linéaire gaussien.\
Rappelons que nous avions conclu, avant transformation, que l'hypothèse d'homoscédasticité des résidus était vérifiée. Cependant, ce n'était pas le cas pour l'hypothèse de normalité des résidus. Regardons ce qu'il en est après transformation.

```{r, echo=F, warning=F, fig.cap="QQ-plot : à gauche sur données prop, à droite sur données prop après transformation Arcsin"}
lm.prop.asin <- lm(prop.asin~.-pa-prop-totalabund-Galumna, data =mites)
par(mfrow=c(1,2))
plot(x=lm.tot.prop, which = 2)
title(main="lm(prop~.)", line=1.4)
plot(lm.prop.asin, which =2)
title(main="lm(Arcsin(prop)~.)", line=1.4)
```

Graphiquement, la transformation $arcsin$ ne semble pas avoir amélioré la normalité des résidus. Vérifions avec un test de Shapiro-Wilk :

```{r, echo=F}
cat("p-value Shapiro-Wilk Test :",shapiro.test(lm.prop.asin$residuals)$p.value)
```

Une nouvelle fois, on rejette $H_0$ et on conclut que l'hypothèse de normalité des résidus n'est toujours pas vérifiée, même après une transformation $arcsin$.

#### Transformation de Box-Cox

 

La tranformation de Box & Cox est une transformation non linéaire souvent utilisée pour rendre les données normales. L'objectif est donc d'obtenir une distribution normale des données après transformation, et une variance constante. La transformation s'obtient comme suit : $$
\{ y \in \mathbb{R}^+_*, \lambda \in \mathbb{R} \} : y^*=f(y, \lambda)= 
\left\{
\begin{array}{ll}
\frac{y^{\lambda} - 1}{\lambda} \hspace{2em} (\lambda \neq 0) \\
\log(y) \hspace{2em} (\lambda = 0)
\end{array}
\right.
$$Dans le cas où $\lambda=0$, on retrouve une transformation logarithmique classique. Dans le cas où $\lambda=1$, cela revient à ne pas faire de transformation et de conserver notre variable d'origine à une translation près. Pour un échantillon de $n$ observations, on applique cette même transformation à chaque valeur. Attention, la transformation n'est pas définie pour les valeurs négatives ou nulles de la variable. Dans ce cas de figure, Box & Cox proposent de translater toutes les valeurs :

$$
\{ y \in \mathbb{R}^+_*, \lambda_1 \in \mathbb{R}, \lambda_2 \in \mathbb{R} \} : y^*=f(y, \lambda_1, \lambda_2)= 
\left\{
\begin{array}{ll}
\frac{(y+\lambda_2)^{\lambda_1} - 1}{\lambda} \hspace{2em} (\lambda_1 \neq 0) \\
\log(y+\lambda_2) \hspace{2em} (\lambda_1 = 0)
\end{array}
\right.
$$Il faut choisir $\lambda_2$ tel que $\forall y, y+\lambda_2>0$. Dans notre cas, on fixera $\lambda_2=1$ et on utilisera la fonction `MASS::boxcox` pour estimer $\lambda_1$. Appliquons maintenant cette transformation à chacune de nos variables réponses, puis re-vérifions les hypothèses du modèle linéaire gaussien, et plus particulièrement l'hypothèse de normalité qui était rejetée pour nos trois modèles et qui est supposée avoir été améliorée par la transformation.

**Variable réponse Galumna :**

```{r,echo=F}
lm.tot.g.translat <- lm((Galumna+1)~.-pa-prop-totalabund, data=mites)
bc <- boxcox(lm.tot.g.translat, lambda=seq(-3,2,0.1), plotit = T)
lambda <- bc$x[which.max(bc$y)]
cat("Lambda_1 optimal : ", lambda)
```

Si la valeur $0$ avait été contenue par l'intervalle de confiance pour $\lambda_1$, il aurait été judicieux de simplement effectuer une transformation logarithmique. Ici, ce n'est pas le cas, on va donc réajuster un modèle sur les données transformées par Box & Cox avec le $\lambda_1$ qui maximise la log-vraisemblance (ici $\lambda_1=-1.8$).

```{r, echo=F, warning=F, fig.cap="QQ-plot : à gauche sur données Galumna, à droite sur données Galumna après transformation Box Cox"}
lm.tot.g.bc <- lm(((Galumna+1)^lambda-1)/lambda ~.-pa-prop-totalabund, data=mites)
par(mfrow=c(1,2))
plot(lm.tot.g, which = 2)
title(main="lm(Galumna~.)", line=1.4)
plot(lm.tot.g.bc, which = 2)
title(main="lm(BoxCox(Galumna)~.)", line=1.4)
```

```{r, echo=F}
cat("p-value pour le test de Shapiro-Wilk : ",shapiro.test(lm.tot.g.bc$residuals)$p.value)
```

Graphiquement, la transformation de Box & Cox ne semble pas avoir amélioré la normalité des résidus. Pour le test de Shapiro-Wilk, on obtient une p-value inférieure à 5% donc on rejette l'hypothèse de normalité des résidus. La transformation de Box & Cox pour la variable Galumna n'a pas réglé le problème des hypothèses du modèle linéaire gaussien non vérifiées.

\newpage

**Variable réponse pa :**

```{r,echo=F}
lm.tot.pa.translat <- lm((pa+1)~.-Galumna-prop-totalabund, data=mites)
bc <- boxcox(lm.tot.pa.translat, lambda=seq(-5,2,0.1), plotit = T)
lambda <- bc$x[which.max(bc$y)]
cat("Lambda_1 optimal : ", lambda)
```

On réajuste un modèle sur les données transformées par Box & Cox avec $\lambda_1=-2.6$.

```{r, echo=F, warning=F, fig.cap="QQ-plot : à gauche sur données pa, à droite sur données pa après transformation Box Cox"}
lm.tot.pa.bc <- lm(((pa+1)^lambda-1)/lambda ~.-pa-prop-totalabund, data=mites)
par(mfrow=c(1,2))
plot(lm.tot.pa, which = 2)
title(main="lm(pa~.)", line=1.4)
plot(lm.tot.pa.bc, which = 2)
title(main="lm(BoxCox(pa)~.)", line=1.4)
```

```{r, echo=F}
cat("p-value pour le test de Shapiro-Wilk : ",shapiro.test(lm.tot.pa.bc$residuals)$p.value)
```

Une nouvelle fois, graphiquement la transformation ne semble pas avoir amélioré la normalité des résidus. Pour le test de Shapiro-Wilk, on obtient une p-value très faible donc on rejette une fois de plus l'hypohtèse de normalité des résidus.

\newpage

**Variable réponse prop :**

```{r,echo=F}
lm.tot.prop.translat <- lm((prop+1)~.-pa-Galumna-totalabund, data=mites)
bc <- boxcox(lm.tot.prop.translat, lambda=seq(-200,0,0.1), plotit = T)
lambda <- bc$x[which.max(bc$y)]
cat("Lambda_1 optimal : ", lambda)
```

On réajuste un modèle sur les données transformées par Box & Cox avec $\lambda_1=-147$. Il s'agit d'une valeur de $\lambda_1$ très élevée comparée aux valeurs précédents, mais c'est celle qui maximise la log-vraisemblance.

```{r, echo=F, warning=F, fig.cap="QQ-plot : à gauche sur données prop, à droite sur données prop après transformation de Box Cox"}
lm.tot.prop.bc <- lm(((prop+1)^lambda-1)/lambda ~.-pa-Galumna-totalabund, data=mites)
par(mfrow=c(1,2))
plot(lm.tot.prop, which = 2)
title(main="lm(prop~.)", line=1.4)
plot(lm.tot.prop.bc, which = 2)
title(main="lm(BoxCox(prop)~.)", line=1.4)
```

```{r, echo=F}
cat("p-value pour le test de Shapiro-Wilk : ",shapiro.test(lm.tot.g.bc$residuals)$p.value)
```

Graphiquement, la transformation semble avoir légèrement amélioré la normalité des résidus (ce n'est pas la même échelle pour $y$ pour les deux graphiques). Cependant, d'après le test de Shapiro-Wilk, on rejette l'hypothèse de normalité. \newpage 

**Conclusion :**

Dans cette première partie de l'analyse du jeu de données "mites", nous avons tenté d'ajuster un modèle linéaire gaussien sur trois variables explicatives. Cependant, l'une des difficultés lorsque l'on ajuste un modèle linéaire est qu'il doit vérifier un certain nombre d'hypothèses. Dans notre cas, il y avait toujours au moins une hypothèse qui n'était pas vérifiée. Nous avons donc mis en place des transformations de données, adaptées à nos variables, afin que le modèle vérifie les hypothèses. Malheureusement, ces transformations n'ont pas permis de valider les hypothèses, en particulier celle de normalité des résidus.\
Dans la pratique, il existe de nombreuses autres distributions de probabilité pour décrire les données : Poisson, Bernouilli, Binomiale... Nous allons donc essayer une autre approche pour ajuster un modèle sur nos données : les modèles linéaires généralisés.\
Un modèle linéaire généralisé a quatre composantes :

1.  les variables réponses $Y_1, Y_2,…, Y_n$ sont supposées suivre la même distrubtion appartenant à la **famille exponentielle**

2.  un vecteur de paramètres $\beta$ à estimer et des variables explicatives $X=[1,x^ 1,…,x^p]$

3.  une fonction de lien $g$ telle que\
    $g(\mu_i) = x_i^T\beta$ avec $E(Y_i)=\mu_i$

4.  une fonction de variance $V(\mu_i)$ qui décrit comment la variance $Var(Y_i)$ dépend de l'espérance $E(Y_i)=\mu_i$: $$Var(Y_i)=\phi V(\mu_i)$$ où $\phi$ est un paramètre de dispersion ou facteur d'échelle (constante) et $V$ une fonction de variance.\
    Finalement, le modèle linéaire gaussien est un cas particulier du modèle linéaire généralisé où l'on a $Y=X\beta + \epsilon$ avec les $\epsilon_i$ i.i.d. suivant $N(0,\sigma^2)$. Dans ce cas, $\mu_i=E(Y_i)=x_i^T\beta$ et la fonction de lien $g$ est la fonction identité $g(\mu_i)=\mu_i=x_i^T\beta$.

Sous R, la fonction utilisée pour les modèles linéaires généralisés est `glm`.

Nous allons donc essayer des modèles linéaires généralisés avec différentes distributions et fonctions de lien pour nos données :

-   une régression logistique binaire sur la variable `pa`

-   une régression logistique sur la variable `prop`

-   une régression de Poisson sur la variable `Galumna`

\newpage

## GLM Régression Logistique binaire sur la variable d'occurrence (pa)

**Objectif** : expliquer l'occurence `pa`, déterminer le meilleur modèle, interpréter les coefficients, puis évaluer l'ajustement de ce modèle et son pouvoir prédictif.

La variable binaire est un type de variable commun dans les données en écologie ou en santé : on observe un phénomène $Y$ ou son absence. Dans notre cas, on veut savoir si l'occurence de mites Galumna varie en fonction de l'environnement.

Ici, le modèle linéaire généralisé est composé de :

1.  la variable réponse $Y_i, i=1,...,n$ qui suit une distribution de Bernouilli de paramètre $\pi_i$

2.  un vecteur de paramètre $\beta$ à estimer et des variables explictives $X=[1,x^1,…,x^p]$

3.  la fonction de lien $g$ nommée $logit$. Elle est définie de $[0,1]$ sur $\mathbb{R}$ par $g(\pi)=ln(\frac{\pi}{1-\pi})$, soit $\pi_i=g^{-1}(x_i^T\beta)=\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)}$

Ainsi, on suppose ici que $y_i$ suit une loi Binomiale $B(1,\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)})$.

Sous R, on indique en paramètre de la fonction `family="binomial"` et il n'est pas nécessaire de préciser la fonction de lien car par défaut, pour la famille binomiale, la fonction de lien est $logit$.

### Recherche du meilleur modèle

```{r, echo=F}
mites$pa <- as.factor(mites$pa)
```

-   **Méthode "à la main"**

Pour débuter, on construit le modèle complet (avec toutes les variables explicatives) pour déterminer lesquelles sont significatives :

```{r, echo=F}
glm.tot.pa <- glm(pa~WatrCont + SubsDens + Topo + Shrub + Substrate, data=mites, family = "binomial")
summary(glm.tot.pa)
```

Lorsque l'on effectue le `summary` du modèle complet, on obtient un warning qui nous dit qu'une ou plusieurs observations du jeu de données sont prédites à 0 ou 1, ce qui ne veut pas forcément dire que la régression logistique est mauvaise. On rappelle que dans notre jeu de données on avait des proportions nulles. On décide donc d'ignorer le warning. La sortie du `summary` nous permet de voir quelles variables sont significatives pour le test de Wald. Celui-ci permet de tester la nullité d'un coefficient : $H_0 : \beta_j=0$. Sous $H_0$, on a : $$\sqrt{n}(I(\hat{\beta}))_{jj}^{\frac{1}{2}}(\hat{\beta}_j)=\frac{\hat{\beta}_j}{\sqrt{\hat{Var}(\hat{\beta}_j)}}\sim N(0,1)$$ Ici, les variables significatives semblent être WatrCont, SubsDens et Topo.

On peut alors tester un modèle uniquement avec celles-ci :

```{r, echo=F}
glm.best.pa <- glm(pa~WatrCont + SubsDens + Topo, data=mites, family = "binomial")
summary(glm.best.pa)
```

Ce modèle semble mieux car les variables sont toutes vraiment significatives.

Afin de s'assurer de la significativité du retrait des autres variables, nous pouvons réaliser un test de modèles emboîtés. L'idée est de tester le modèle complet $$Y=\beta_0 + \beta_1 X_1 +...+\beta_p X_p + \epsilon$$ contre le sous-modèle $$Y=\beta_0 + \beta_1 X_1 +...+\beta_{p-q} X_{p-q} + \epsilon$$ dans lequel on n'a pas pris en compte les $q$ dernières variables. Cela revient à tester dans le modèle global $H_0 : \beta_{p-q+1}=...=\beta_p=0$ contre $H_1$ : le contraire. Si $rg(X)=p$ et $\epsilon \sim N(0,\sigma^2 I_n)$ alors sous $H_0$ :\
$$F=\frac{n-p}{q} \frac{SCR_c-SCR}{SCR} \sim F(q,n-p)$$ où $SCR$ est la somme des carrés des résidus dans le modèle complet et $SCR_c$ la somme des carrés des résidus du sous-modèle. On l'effectue avec la fonction `stats::anova`.

```{r, echo=F}
anova(glm.best.pa, glm.tot.pa, test = 'Chisq')
```

La grande p-value nous indique que les variables présentes dans le plus grand modèle mais pas dans les plus petit ne sont pas significatives. Avant de confirmer le choix de ce modèle, regardons les valeurs du critère BIC (critère explicatif) pour chacun des deux modèles.

```{r, echo=F}
kable(data.frame(BIC(glm.best.pa),BIC(glm.tot.pa)), col.names = c("Petit modèle", "Grand modèle"), caption = "BIC", position="H")
```

En accord avec les résultats précédents, le critère BIC est inférieur pour le petit modèle. Cela confirme notre choix de garder uniquement les variables `WatrCont`, `SubsDens` et `Topo.`

Nous pouvons comparer ce modèle obtenu avec celui déterminé par une sélection automatique.

-   **Méthode de sélection automatique**

Nous effectuons une sélection stepwise descendante, toujours avec le critère BIC (option `k=log(n)` dans la fonction `step`). Celle-ci part du plus gros modèle et tente d'enlever les variables les moins significtives afin de retrouver le meilleur modèle.

```{r, include=F}
n <- nrow(mites)
glm.tot.pa.step <- step(glm.tot.pa, direction = "backward", k = log(n))
```

```{r, echo=F}
cat("Modèle retenu par la fonction step : ")
print(glm.tot.pa.step)
```

La sélection automatique retient également le modèle contenant les variables `WatrCont`, `SubsDens` et `Topo` car c'est celui pour lequel le BIC est le plus faible.

Par conséquent, nous retenons le modèle suivant pour la suite :

```{=tex}
\begin{equation}
  logit(\hat{\pi})= - 0.583 - 0.022*WatrCont +0.173*SubsDens + 2.738*TopoHummock (\#eq:a)
\end{equation}
```
### Interprétation des coefficients

La sortie de notre modèle indique que le contenu en eau, la densité du substrat et la topographie sont associés significativement à l'occurrence de mites. Cependant, on peut également interpréter les coefficients de la pente grâce aux odds-ratio. L'odd-ratio est défini comme le rapport des probabilités d'apparition de l'évènement $Y=1$ contre $Y=0$. L'odd est la quantité $\frac{\pi_i}{1-\pi_i}$. Dans le modèle logistique, on définit plus précisément l'odd par : $$\frac{\pi_i}{1-\pi_i}=\exp(x_i^T\beta)=\exp(\beta_0+\beta_1x^i_1+...+\beta_px_i^p)$$ et si on considère deux individus $i_1$ et $i_2$ dont la valeur des covariables ne diffère que de 1 pour la $j$-ième covariable, soit $x_{i_1}^j-x_{i_2}^j=1$, on calcule l'odd-ratio avec : $$\frac{\pi_{i_1}}{1-\pi_{i_1}}/\frac{\pi_{i_2}}{1-\pi_{i_2}} = \exp(\beta_j)$$ On dira alors qu'une augmentation de 1 de la variable $j$ entraîne une multiplication de l'odds-ratio de $\exp(\beta_j)$.

```{r, echo=F}
kable(data.frame(exp(glm.best.pa$coefficients),row.names=c("Intercept", "WatrCont", "SubsDens", "TopoHummock")), col.names=c("Odds-ratios"), caption="Odds-ratios du modèle expliquant pa", position ="H")
```

Grâce à ces coefficients, nous pouvons déterminer le pourcentage de probabilité de la présence de mites, lorsque que l'on augmente l'une des variables. En effet, pour chaque augmentation d'une unité du contenu en eau (`WatrCont`), nous avons $2.2$% ($0.978*100 - 100$) de risque en moins d'avoir une présence de mites, tandis ce que pour une unité de densité du substrat (`SubsDens`) en plus, le risque est plus élevé de $18$%. Enfin, lorsque de la topographie est de type Hummock, il y a énormément de probabilité d'avoir des mites, pusique celle-ci augmente de plus de $1400$%.

### Validation du modèle

Afin de valider notre modèle, nous allons à présent évaluer son ajustement, ainsi que ses pouvoirs explicatifs et de classfication.

-   **Evaluation de l'ajustement du modèle**

    -   Effet levier

    Dans un premier temps, nous pouvons nous intéresser aux points leviers, c'est à dire les points qui influencent fortement sur leur propre estimation.

    ```{r, echo=F, fig.cap="Points leviers du modèle expliquant pa"}
    p <- length(glm.best.pa$coefficients)
    n <- nrow(mites)
    plot(influence(glm.best.pa)$hat, type="h", ylab="h_ii")
    abline(h=c(2*p/n, 3*p/n), col=2)
    ```

    On remarque sur le graphique précédent que quatre observations peuvent être considérées comme points leviers. Une connaissance plus approfondie du jeu de données nous permettrait de comprendre plus en détails ces résultats.

    -   Points influents

    Regardons ensuite les points influents sur le modèle. Ce sont des points importants car leur présence joue beaucoup sur l'estimation des coefficients. Pour les déterminer, on peut représenter leur distance de Cook, c'est à dire la distance entre le vecteur des coefficients estimés avec toutes les observations et celui estimé avec toutes les observations sauf une.

    ```{r, echo=F, fig.cap="Points influents du modèle expliquant pa"}
    plot(cooks.distance(glm.best.pa), type="h", ylab="Distance de Cook")
    ```

    Sur le graphique obtenu, on remarque notamment l'observation 15 qui a une distance de Cook nettement supérieure aux autres. Cela signifie que c'est un point influent. Là encore, il serait necessaire de connaître plus en détails notre jeu de données pour pouvoir l'expliquer et notammant déterminer si c'est un point levier, un point abberrant ou les deux. Nous pouvons tout de même remarquer qu'il figurait déja dans les points à effet levier relevés précédemment.

    -   Analyse des résidus

    Afin de tester la qualité de l'ajustement de notre modèle, on peut utiliser le test de Pearson basé sur les résidus de Pearson. Ce test a pour hypothèse nulle $H_0$ : le modèle ajuste correctement les données. On définit les résidus de Pearson comme $\hat{r}_{P,i}=\frac{y_i-\hat{y_i}}{\sqrt{Var(\hat{y_i})}}$ et sous $H_0$, on a $\sum_{i=1}^{n}\hat{r}_{P,i}^2 \sim \chi^2(n-(p+1))$.

    ```{r, echo=F}
    res <- sum(residuals(glm.best.pa, type = "pearson")^2)
    ddl <- df.residual(glm.best.pa)
    pvalue <- 1 - pchisq(res,ddl)
    cat("p-value pour le test de Pearson : ", pvalue)
    ```

    La p-value étant très grande, on admet que le modèle est très bien adapté aux données. On peut tout de même visualiser ces résidus de Pearson.


    ```{r, echo=F, fig.cap="Résidus de Pearson du modèle expliquant pa"}
    res2 <- residuals(glm.best.pa, type="pearson")
    plot(res2, ylab="Residuals")
    abline(h=c(-2,2), col=2)
    ```

    Nous remarquons deux individus ayant des résidus élevés. Une nouvelle fois, nous retrouvons l'individu 15. Il serait peut-être judicieux de l'enlever pour la suite. Cependant, le jeu de données n'étant pas très grand, nous décidons de le conserver afin de ne pas le réduire davantage.

    -   Test d'Hosmer-Lemeshow

    Par ailleurs, on peut également effectuer un test d'Hosmer-Lemeshow permettant d'évaluer la pertinence d'un modèle de régression logistique pour lequel on a des données individuelles (pour plus d'informations voir [@Hosmer2000])

    ```{r, echo=F}
    pvalue <- hoslem.test(as.numeric(as.character(mites$pa)),fitted(glm.best.pa))$p.value
    cat("p-value pour le test de Hosmer-Lemeshow : ", pvalue)
    ```

    De même, la très grande p-value nous indique que le modèle est bien adapté aux données.

    Avant de valider définitivement le choix de ce modèle, il faut évaluer son pouvoir explicatif ainsi que son pouvoir de classification.

-   Evaluation du pouvoir explicatif du modèle

Afin d'évaluer le pouvoir explicatif de notre modèle, on peut calculer son Pseudo-$R^2$. Il en existe plusieurs. Ici, nous allons déterminer celui de McFadden (1973) correspondant au quotient entre la différence des déviances nulles et résiduelles et la déviance nulle.

```{r, echo=F}
pseudoR2 <- ((glm.best.pa$null.deviance-glm.best.pa$deviance)/glm.best.pa$null.deviance)
cat("pseudo-R2 : ", pseudoR2)
```

D'après ce résultat, on en conclut que notre modèle explique plus de $62.2$% des données. Regardons ensuite s'il est capable de bien classer ces données.

-   Evaluation du pouvoir de classification du modèle

Pour évaluer le pouvoir de classification de notre modèle, nous allons juger ses prédictions grâce à une courbe ROC. Afin de ne pas prédire sur des individus ayant servis à la construction du modèle, nous effectuons une validation croisée. En effet, pour chaque individu $i$, nous estimons le modèle sans cet individu puis nous prédisons la valeur de `pa` pour celui-ci. Nous obtenons alors les prédictions de `pa` pour tous les individus et nous pouvons tracer la courbe ROC.

```{r, echo=F, fig.cap="Courbe ROC"}
pred <- 1:n
for (i in 1:n){
  # On estime le modèle sans i (on l'enlève lorsqu'on estime le modèle)
  fit <- glm(pa~WatrCont + SubsDens + Topo, data=mites, family="binomial", subset=-i)
  # On prédit la proba asscoiée à i
  pred[i] <- predict(fit,mites[i,-2], type="response")
}

pr <- prediction(pred,mites$pa)
roc <- performance(pr, measure="tpr",x.measure="fpr")
plot(roc)
```

Cette courbe résume le taux de vrais positifs en fonction du taux de faux positifs. Elle est d'autant meilleure qu'elle s'éloigne de la diagonale. Ici, nous avons une courbe largement au dessus de la diagonale. Nous pouvons d'ailleurs calculer son aire sous la courbe (AUC).

```{r, echo=F}
perf <- performance(pr, "auc")
cat("AUC : ", perf@y.values[[1]])
```

L'aire sous la courbe est proche de 1 : le pouvoir prédictif de notre modèle est alors très bon.

Enfin, nous pouvons résumer les bonnes et mauvaises prédictions de notre modèle grâce à une matrice de confusion. Pour cela, il est intéressant de déterminer en amont le seuil à partir duquel on admet la présence de mites. On trace alors la courbe du taux d'erreur en fonction des différents seuils.

```{r, echo=F, fig.cap="Taux d'erreur en fonction du seuil"}
res.err <- performance(pr,measure = "err")
plot(res.err)
```

Nous remarquons que le taux d'erreur le plus faible est pour un seuil de 0.5. C'est alors le seuil que nous allons choisir pour étblir notre matrice de confusion.

```{r, echo=F}
table(pred>0.5, mites$pa)
```

D'après cette matrice, nous avons prédit quarante fois l'abscence de mites et vingt-trois fois leur présence en ayant raison. Cependant, le modèle s'est trompé sept fois : cinq fois en ayant prédit l'abscence de mites à tort et deux fois leur présence à tort. Par conséquent, le taux d'erreur est de $\frac{7}{70}=0.1$. On peut en conclure que ce modèle a de bonnes qualités de classification.

**Conclusion :** Nous en déduisons que le modèle \@ref(eq:a) de régression logistique expliquant l'occurrence de mites grâce au contenu en eau, à la densité du substrat et à la topographie obtient de bons résultats, tant sur son ajustement que sur son pouvoir de classification.

\newpage

## GLM Régression Logistique sur les données agrégées (prop)

**Objectif** : expliquer la fréquence relative `prop`, déterminer le meilleur modèle pour cette variable réponse, interpréter les coefficients, puis évaluer l'ajustement de ce modèle.

La variable `prop` est, comme son nom l'indique, une variable en proportion. Malgré qu'il ne s'agisse pas d'une variable binaire, ce cas est proche d'une régression logistique. En effet, cette fois-ci, au lieu de prédire une valeur binaire échec ou succès (0 ou 1), on veut prédire la proportion de succès $P_i=Y_i/n_i$. Dans notre cas, la valeur $n_i$ est la variable `totalabund` qui représente le nombre total de mites, et la variable $Y_i$ est le nombre de succès, ici le nombre de mites qui sont des Galumna (représenté par la variable `Galumna` elle-même). En clair, nous avons : `prop` = `Galumna/totalabund`. On a $\mathbb{E}(Y_i)=n_i \pi_i$ et donc $\mathbb{E}(P_i)=\pi_i$. Et on modélise les probabilités $\pi_i$ par $g(\pi_i)=x_i^T \beta$ où $x_i$ est le vecteur des variables explicatives, $\beta$ est un vecteur de paramètres et $g$ est la fonction de lien. La fonction de lien pour la régression logistique est la fonction $logit$.

Sous R, on peut utiliser la fonction `glm` de manière semblable à lorsque l'on fait une régression logistique classique, mais en précisant cette fois des poids a priori.

### Recherche du meilleur modèle

On suivra la même méthode que dans la partie précédente pour chercher le meilleur modèle : une recherche à la main dans un premier temps, puis une méthode de sélection automatique.

-   **Méthode "à la main"**

```{r, echo=F}
prop.reg.tot <- glm(prop~WatrCont + SubsDens + Topo + Shrub + Substrate, data=mites, family = "binomial", weights = totalabund)
summary(prop.reg.tot)
```

On voit qu'il n'y que très peu de variables significatives. On créé un modèle avec les deux variables les plus significatives du modèle complet : `WatrCont` et `SubsDens`.

```{r, echo=F}
prop.reg.2 <- glm(prop~WatrCont + SubsDens, data=mites, family = "binomial", weights = totalabund)
summary(prop.reg.2)
```

Cette fois-ci on voit que toutes les variables, ainsi que la constante sont significatives pour le test de Wald. Pour rappel, ce test permet de tester la nullité d'un coefficient : $H_0 : \beta_j=0$.

-   **Méthode de sélection automatique**

On effectue maintenant une recherche automatique du meilleur modèle en partant du plus gros modèle et en se basant sur le critère BIC, critère explicatif.

```{r, include=F}
prop.reg.back<-step(prop.reg.tot, direction="backward", k=log(nrow(mites)))
```

```{r, echo=F}
print("Modèle retenu par la fonction step: ")
print(prop.reg.back)
```

Avec une procédure de sélection backward, on garde 3 variables : `WatrCont`, `SubsDens` et `Topo`.

Pour cette étude, on se place dans un cadre plus explicatif que prédictif. On décide donc de comparer ces trois modèles avec le BIC :

```{r, echo=F}
kable(data.frame(BIC(prop.reg.tot),BIC(prop.reg.2),BIC(prop.reg.back)), col.names = c("Modèle complet", "Modèle 2 variables", "Modèle 3 variables"), caption = "BIC des différents modèles", position="H")
```

Le modèle à trois variables est celui qui minimise le BIC. On peut tout de même effectuer un test de modèles emboîtés afin de regarder si la troisième variable `Topo` est bien significative. On teste donc le modèle à 2 variables contre celui à 3 variables.

```{r, echo=F}
anova(prop.reg.2, prop.reg.back,test="Chisq")
```

La p-value est inférieure à 0.05 donc la variable `Topo` est significative.

On effectue un summary du modèle à 3 variables.

```{r, echo=F}
prop.reg.best <- prop.reg.back
summary(prop.reg.best)
```

On remarque que toutes les variables sont significatives d'après le test de Wald.

On retient finalement le modèle suivant :\
\begin{equation}
logit(\hat{\pi})=-4.23 -0.006*WatrCont+0.029*SubsDens+0.729*TopoHummock (\#eq:b)
\end{equation}

### Interprétation des coefficients

On peut interpréter les coefficients du modèle pour nos trois variables grâce à leurs odds-ratios. L'odd-ratio pour la variable $j$ est égal à $\exp(\beta_j)$.

```{r, echo=F}
kable(data.frame(exp(prop.reg.best$coefficients),row.names=c("Intercept", "WatrCont", "SubsDens", "TopoHummock")), col.names=c("Odds-ratios"), caption="Odds-ratios du modèle expliquant prop", position="H")
```

L'odd-ratio pour la variable `WatrCont` est inférieur à 1. De plus, sa p-value associée au test de Wald est inférieure à 5% donc on peut dire que la proportion de mites est significativement moins élevée si la quantité d'eau dans le sol augmente. L'odd-ratio de `SubsDens` est quant à lui supérieur à 1, et sa p-value pour le test de Wald inférieure à 5%. Cela signifie que la proportion de mites sera significativement plus importante si la densité du substrat augmente.\
Enfin, l'odd-ratio pour `Topo` vaut 2.07 et sa p-value est inférieure à 5%. Donc on peut conclure que si la topographie est de type Hummock, la proportion de mites sera significativement plus élevée.

### Validation du modèle

Enfin, nous devons maintenant regarder si notre modèle est "bon". Nous allons, pour cela, évaluer sa qualité d'ajustement et son pouvoir explicatif.

-   **Evaluation de l'ajustement du modèle**

    -   Effet levier

    On regarde dans un premier temps les points leviers (qui influencent fortement leur estimation).

    ```{r, echo=F, fig.cap="Points leviers du modèle expliquant prop"}
    p<-length(prop.reg.back$coefficients)
    plot(influence(prop.reg.back)$hat, type="h", ylab="h_ii")
    abline(h=c(2*p/n, 3*p/n), col=2)
    ```

    On voit que 5 observations peuvent être déclarées comme "points leviers".

    -   Points influents

    Pour les observer, on représente leur distance de Cook.

    ```{r, echo=F, fig.cap="Points influents du modèle expliquant prop"}
    plot(cooks.distance(prop.reg.back), type="h", ylab="Distance de Cook", main="Points influents")
    ```

    On relève plusieurs points influents. Notamment la 8ème observation, qui était déjà un point levier. Généralement, si un point est influent, il est soit levier, soit aberrant, soit les deux.

    -   Analyse des résidus

    Finalement, nous allons analyser les résidus. On effectue pour cela un test de Pearson ou test global de la qualité de l'ajustement basé sur les résidus de Pearson.

    ```{r, echo=F}
    res <- sum(residuals(prop.reg.back, type = "pearson")^2)
    ddl <- df.residual(prop.reg.back)
    pvalue <- 1-pchisq(res,ddl)
    cat("p-value pour le test de Pearson : ", pvalue)
    ```

    La p-value est très petite. Cela signifie que d'après le test de Pearson, notre modèle n'ajuste pas très bien les données. Cependant, nous ne voyons pas comment le changer afin qu'il soit meilleur à ce niveau là. Nous continuons avec ce modèle, tout en gardant à l'esprit qu'il présente des défauts d'ajustement.

    On finit par représenter les résidus.

    ```{r, echo=F, fig.cap="Résidus de Pearson du modèle expliquant prop"}
    res<-residuals(prop.reg.back, type="pearson")
    plot(res, ylim=c(-3,5), ylab="Residuals")
    abline(h=c(-2,2), col=2)
    ```

    On voit que les résidus sont répartis de façon homogène autour de l'axe des abscisses. Ils sont pour la plupart proches de 0. Cependant, trois points semblent avoir une valeur de résidu élevée. Peut-être correspondent-ils à des points leviers ou aberrants observés précédemment. Nous pourrions les enlver du jeu de données afin de construire un modèle qui ajusterait encore mieux les données mais nous décidons de les garder car le jeu de données ne contient pas beaucoup d'individus (70) et nous ne voulons pas le réduire davantage. De plus, nous n'avons pas une connaissance très poussée de nos données donc nous ne pourrions pas justifier le retrait de ces individus pour notre étude.

-   **Evaluation du pouvoir explicatif du modèle**

Pour finir la validation du modèle, on calcule le pseudo-$R^2$, qui nous donne la variance expliquée par notre modèle :

```{r, echo=F}
pseudoR2 <- ((prop.reg.best$null.deviance-prop.reg.best$deviance)/prop.reg.best$null.deviance)
cat("pseudo-R2 : ", pseudoR2)
```

Notre modèle explique $44$% de la variance. Ce score n'est pas très élevé. Cependant, en régression logistique, les valeurs faibles de pseudo-$R^2$ sont souvent courantes.

**Conclusion :** Nous en déduisons que le modèle \@ref(eq:b) de régression logistique expliquant la proportion de mites Galumna grâce au contenu en eau, à la densité du substrat et à la topographie obtient de bons résultats.

\newpage

## GLM Régression Poisson sur la variable d'abondance (Galumna)

**Objectif** : modéliser l'abondance de l'espèce Galumna en fonction des caractéristiques du substrat (son contenu en eau `WatrCont` et sa densité `SubsDens`) et, si nécessaire, des autres variables environnementales.

Pour ce faire, nous allons utiliser un modèle linéaire généralisé avec une distribution adaptée à notre problème de comptage : la distribution de Poisson.

Pour une distribution Poisson $Y \sim Pois(\lambda)$ avec le lien $log$ par défaut nous avons $log(Y)=\eta(.)$ avec l'inverse de ce lien $\lambda=e^{\eta(.)}$, où $\eta(.)$ représente le prédicteur linéaire.

Comme pour la régression logistique, la régression de Poisson utilise la fonction `glm`. Il faut spécifier la famille "poisson" et (optionnellement) le lien $log$. En effet, le logarithme est la foncton de lien par défaut pour la régression de Poisson sur R.

### Recherche du meilleur modèle

-   **Méthode "à la main"**

Nous allons commencer par ajuster un modèle complet et ensuite effectuer une sélection de variables.

```{r, echo=F}
glm.tot.poi <- glm(Galumna~WatrCont + SubsDens + Topo + Shrub + Substrate, data=mites, family = poisson(link='log'))
summary(glm.tot.poi)
```

Nous observons dans le `summary` de notre modèle complet qu'il n'y a que très peu de variables significatives. On créé un modèle avec les deux variables significatives du modèle complet : `WatrCont` et `SubsDens`.

```{r, echo=F}
glm.poi <- glm(Galumna~WatrCont + SubsDens, family = poisson(link = "log"), data = mites )
summary(glm.poi)
```

On observe que toutes les variables, ainsi que la constante, sont significatives pour le test de Wald.

-   **Recherche automatique**

Avant de confirmer ce modèle, nous allons effectuer une recherche automatique du meilleur modèle en partant du plus gros modèle et en se basant sur le critère BIC, critère explicatif.

```{r, include=F}
glm.reg.back<-step(glm.tot.poi, direction="backward", k=log(nrow(mites)))
```

```{r, echo=F}
print("Modèle retenu par la fonction step : ")
print(glm.reg.back)
```

Avec une procédure de sélection backward, on garde 3 variables : `WatrCont`, `SubsDens` et `Topo`.

Nous avons donc un nouveau modèle :

```{r, echo=F}
glm.poi3 <- glm(Galumna~WatrCont + SubsDens + Topo, family = poisson(link = "log"), data = mites )
summary(glm.poi3)
```

On voit que toutes les variables sont significatives pour le test de Wald.

Pour cette étude, on se place dans un cadre plus explicatif que prédictif. On décide donc de comparer ces trois modèles avec le BIC :

```{r, echo=F}
kable(data.frame(BIC(glm.tot.poi),BIC(glm.poi),BIC(glm.poi3)), col.names = c("Modèle complet", "Modèle à 2 variables", "Modèle à 3 variables"), caption = "BIC des différents modèles", position="H")
```

Le modèle à trois variables est celui qui minimise le BIC. On peut tout de même effectuer un test de modèles emboîtés afin de regarder si la troisième variable `Topo` est bien significative. On teste donc le modèle à 2 variables contre celui à 3 variables.

```{r, echo=F}
anova(glm.poi3, glm.poi,test="Chisq")
```

La p-value est inférieure à 0.05 donc la variable `Topo` est significative.

Le modele sélectionné s'écrit donc : \begin{equation}
log(Y)=0.710-0.008*WatrCont+0.034*SubsDens + 0.892*TopoHummock (\#eq:c)
\end{equation}

### Interprétation des coefficients

Nous allons maintenant interpréter les coefficients de notre modèle. Nous allons determiner l'effet de chaque coefficient du modèle sur le prédicteur linéaire $\eta=\beta_0+\beta_1*WatrCont+\beta_2*SubsDens+\beta_3*TopoHummock$.

-   Le coefficient `WatrCont` qui indique le contenu en eau, indique que $\eta$ diminue de 0.008 pour chaque augmentation d'une unité de `WatrCont` si la variable `SubsDens` reste constante. Nous avons donc le facteur $e^{-0.008}=0.99$ qui correspond à une perte de 1% de l'abondance de Galumna par unité de contenu en eau (`WatrCont`) supplémentaire.
-   Le coefficient `SubsDens` qui indique la densité, a un changement additif de $0.023$ qui correspond au facteur $e^{0.034}=1.034585$. On en conclut que la densité n'influe pas sur l'abondance de Galumna.
-   Le coefficient `TopoHummock` qui représente la topographie de type Hummock, a un changement additif de $0.892$ qui correspond au facteur $e^{0.892}=2.440005$. Cela correspond a une augmentation de $250$% de l'abondance de Galumna pour une topographie de type Hummock.

### Validation du modèle

Enfin, nous passons à la phase de validation du modèle. Nous allons, pour cela, regarder s'il y a de la surdispersion dans ce modèle, puis évaluer sa qualité d'ajustement et son pouvoir explicatif.

-   **Analyse de la surdispersion**

Pour une distribution de Poisson, $Var(Y)=\mu=E(Y)$. En pratique, on constate que la variance des données dépasse souvent $\mu$, indiquant une surdispersion dans les paramètres du modèle.\
Par définition, lorsque la déviance résiduelle est supérieure au nombre de degrés résiduels, le modèle est surdispersé. On peut estimer un paramètre de surdispersion $$\phi = \frac{\text{Déviance résiduelle}}{\text{Degré de liberté résiduels}}$$ Nous avons ici :

```{r}
cat("phi = ", glm.poi3$deviance/glm.poi3$df.residual)
```

Cette valeur est bien supérieure à 1 : il y a de la surdispersion dans le modèle. Pour palier cela, nous pouvons utiliser la famille `quasipoisson` dans notre modèle. Corriger pour la surdispersion ne va pas affecter l'estimation des paramètres, mais leur significativité. En fait, les écarts-types des paramètres seront multipliés par $\sqrt{\phi}$. Autrement dit, la surdispersion n'introduit pas de biais, mais augmente l'incertitude sur les valeurs des coefficients.

```{r, echo=F}
glm.poi4 <- glm(Galumna ~ WatrCont + SubsDens + Topo , family = quasipoisson, data = mites )
summary(glm.poi4)
```

On voit que les coefficients ne changent pas, contrairement aux erreurs-types.

-   **Évaluation de l'ajustement du modèle**

    -   Effet levier

    On regarde une nouvelle fois les points leviers.

    ```{r, echo=F, fig.cap="Points leviers du modèle expliquant Galumna"}
    p <- length(glm.poi4$coefficients)
    n <- nrow(mites)
    plot(influence(glm.poi4)$hat, type="h", ylab="h_ii")
    abline(h=c(2*p/n, 3*p/n), col=2)
    ```

    Cinq observations peuvent être considérées comme points leviers. Une connaissance plus approfondie du jeu de données nous permettrait de comprendre plus en détail ces résultats.

    -   Points influents

    Regardons ensuite les points influents sur le modèle.

    ```{r, echo=F, fig.cap="Points influents du modèle expliquant Galumna"}
    plot(cooks.distance(glm.poi4), type="h", ylab="Distance de Cook")
    ```

    Sur le graphique obtenu, on remarque notamment l'observation 8 qui a une distance de Cook nettement supérieure aux autres. Cela signifie que c'est un point influent. Là encore, il serait necessaire de connaître plus en détails notre jeu de données pour pouvoir l'expliquer et notammant déterminer si c'est un point levier, un point abberrant ou les deux.

    -   Analyse des résidus

    Pour tester la qualité de l'ajustement du modèle, on effectue un test de Pearson.

    ```{r, echo=F}
    res <- sum(residuals(glm.poi4, type = "pearson")^2)
    ddl <- df.residual(glm.poi4)
    pvalue <- 1 - pchisq(res,ddl)
    cat("p-value pour le test de Pearson : ", pvalue)
    ```

    La p-value est très petite. Cela signifie que d'après le test de Pearson, notre modèle n'ajuste pas très bien les données. Cependant, nous ne voyons pas comment le changer afin qu'il soit meilleur à ce niveau là. Nous continuons avec ce modèle, tout en gardant à l'esprit qu'il présente des défauts d'ajustement.

-   **Evaluation du pouvoir explicatif du modèle**

Calculons maintenant le Pseudo-$R^2$ du modèle.

```{r, echo=F}
pseudoR2 <- ((glm.poi4$null.deviance-glm.poi4$deviance)/glm.poi4$null.deviance)
cat("pseudo-R2 : ", pseudoR2)
```

D'après ce résultat, on en conclut que notre modèle explique plus de $45$% des données.

Ainsi, nous en déduisons que le modèle \@ref(eq:c) de régression quasi-Poisson expliquant l'abondance de Galumna grâce au contenu en eau, à la densité du substrat et à la topographie est un bon modèle explicatif pour nos données.

**Conclusion :**\
Le but initial de notre étude était de décrire le jeu de données sur les mites Oribatid. Pour cela, nous avons commencé par faire de l'analyse descriptive des variables, individuellement, puis collectivement. Ensuite, nous avons tenté d'ajuster un modèle linéaire pour expliquer trois variables du jeu de données. Nous avons été confrontées à un problème : aucun modèle, même après transformation des données, ne satisfaisait toutes les hypothèses du modèle linéaire gaussien. Nous avons donc choisi d'autres distributions (non gaussiennes), mieux adaptées à nos données, et ajusté un modèle linéaire généralisé pour chacune d'entre elles. Finalement, nous obtenons trois modèles qui nous permettent d'expliquer nos trois variables. Nous jugeons nos modèles plutôt bons, bien que leur qualité globale puisse être discutée.

\newpage

# Données manquantes

## Scénarios NA sur un jeu de données simulées

### Simulations

Dans le but d'étudier différents scénarios d'imputation de données manquantes, nous commençons par simuler $n=100$ réalisations d'un vecteur gaussien $(X,Y)$ de moyenne $\mu=\begin{pmatrix} 0 \\ 0 \end{pmatrix}$ et de variance $\begin{pmatrix} 1 & 0.5 \\ 0.5 & 1 \end{pmatrix}$. Pour cela, on utilise la fonction `rmvnorm` du package `mvtnorm`.

```{r, echo=F}
set.seed(129)
n <- 100
mu <- c(0,0)
matcov <- matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol = 2)
don <- rmvnorm(n=n, mean = mu, sigma = matcov)
colnames(don) <- c("X", "Y") 
```

Ensuite, nous ajoutons des données manquantes sur $Y$ selon plusieurs mécanismes, afin de les comparer par la suite.

-   Mécanisme **MCAR** (**M**issing **C**ompletely **A**t **R**andom)

Le premier mécanisme que nous utilisons est appelé MCAR. Pour celui-ci, les données sont manquantes de façon totalement aléatoire, c'est-à-dire que la probabilité d'abscence est la même pour toutes les observations. Dans notre cas, on choisit $P(M=0)=0.35$.

Voici les informations concernant le jeu de données obtenu :

```{r, echo=F}
set.seed(129)
don.ismcar = don

ismcar <- sample(c(T,F), size = n, prob = c(0.35, 0.65), replace = TRUE)
#Bernouilli trial avec TRUE

don.ismcar[ismcar, "Y"] <- NA
don.ismcar <- as.data.frame(don.ismcar) # création du jeu avec 0.35 de NA pour Y
summary(don.ismcar)
```

-   Mécanisme **MAR** (**M**issing **A**t **R**andom)

Ensuite, nous utilisons le mécanisme MAR. Contrairement au précédent, la probabilité d'abscence est liée à une ou plusieurs autres variables observées. Elle ne dépend cependant pas des valeurs manquantes. Ici, on choisit $P(M=0\vert{X})=\mathcal{B}(\phi(1.2X-0.5))$ où $\phi(.)$ désigne la fonction de répartition d'une loi normale $\mathcal{N}(0,1)$.

On obtient un jeu de données ayant les caractéristiques suivantes pour chacune de ses variables :

```{r, echo=F}
set.seed(129)

don.ismar <- don
ismar <- sapply(don.ismar[,"X"], FUN=function(xx){
  prob <- pnorm(1.2*xx-.5) #Fonction de rep
  res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
  return (res)
})

don.ismar[ismar,"Y"] <- NA #Les valeurs de Y tq ismar = T sont mises à NA
don.ismar <- as.data.frame(don.ismar)
summary(don.ismar)
```

-   Mécanisme **MNAR** (**M**issing **N**ot **A**t **R**andom)

Enfin, nous utilisons le mécanisme MNAR. Cette fois-ci, les données sont manquantes de façon non aléatoire, c'est-à-dire qu'elles dépendent des variables en question. En effet, la probabilité qu'une valeur soit manquante dépend d'une ou plusieurs données non observées pour les autres variables, ici $X$. Dans notre cas, on a $P(M=0\vert{Y})=\mathcal{B}(\phi(1.2X-0.5))$, où $\phi(.)$ désigne toujours la fonction de répartition d'une loi normale $\mathcal{N}(0,1)$.

Le jeu de données obtenu contient toujours deux variables vérifiant les informations suivantes :

```{r, echo = F}
set.seed(129)

don.ismnar <- don
ismnar <- sapply(don.ismnar[,"Y"], FUN=function(xx){
  prob <- pnorm(1.2*xx-.5) #Fonction de rep
  res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
  return (res)
})

don.ismnar[ismnar,"Y"] <- NA #Les valeurs de Y tq ismar = T sont mises à NA
don.ismnar <- as.data.frame(don.ismnar)
summary(don.ismnar)
```

Notons que ce dernier type de données manquantes est plus complexe à traiter par la suite.

### Analyse

Maintenant que nous avons ajouté de trois manières différentes des valeurs manquantes sur la variable $Y$, nous allons analyser les jeux de données obtenus. Pour cela, nous commençons par les visualiser en entier. Nous utilisons la fonction `vis_miss` du package `visdat`.

```{r, echo = F, warning = F, fig.height=4, fig.cap="Pourcentage de données manquantes MCAR"}
vis_miss(don.ismcar) + labs(title="MCAR") + theme(plot.title = element_text(size = 20, color = "black", face = "bold"))
```

```{r, echo = F, warning = F, fig.height=4, fig.cap="Pourcentage de données manquantes MAR"}
vis_miss(don.ismar) + labs(title="MAR") + theme(plot.title = element_text(size = 20, color = "black", face = "bold"))
```

```{r, echo = F, warning = F, fig.height=4, fig.cap="Pourcentage de données manquantes MNAR"}
vis_miss(don.ismnar) + labs(title="MNAR") + theme(plot.title = element_text(size = 20, color = "black", face = "bold"))
```

Nous remarquons que les pourcentages de données manquantes sont globalement équivalents. En effet, elles représentent environ $35$% de la variable $Y$, équivalent à $18$% de la totalité du jeu de données. Cependant, elles ne sont pas réparties de la même façon dans les différents cas.

Nous pouvons alors regarder plus en détails comment sont reparties les données manquantes de $Y$ selon les valeurs de $X$ pour chacun des jeux de données. Pour cela, nous utilisons la fonction `geom_miss_point` disponible dans `ggplot`.

```{r, echo = F, fig.cap="Répartition des données manquantes MCAR"}
ggplot(data = don.ismcar) +
aes(x = X, y = Y) + geom_miss_point(size = 4) + labs(title="MCAR") + theme(plot.title = element_text(size = 20, color = "black", face = "bold"))
```

```{r, echo = F, fig.cap="Répartition des données manquantes MAR"}
ggplot(data = don.ismar) +
aes(x = X, y = Y) + geom_miss_point(size = 4) + labs(title="MAR") + theme(plot.title = element_text(size = 20, color = "black", face = "bold"))
```

```{r, echo = F, fig.cap="Répartition des données manquantes MNAR"}
ggplot(data = don.ismnar) +
aes(x = X, y = Y) + geom_miss_point(size = 4) + labs(title="MNAR") + theme(plot.title = element_text(size = 20, color = "black", face = "bold"))
```

Nous remarquons que pour la deuxième configuration, les valeurs manquantes sont plutôt associées à un $X$ positif, tandis que dans les deux autres cas, elles sont reparties dans les négatifs également.

### Estimation de la moyenne de $Y$ et de l'intervalle de confiance associé

Nous voulons à présent étudier nos jeux de données et déterminer la moyenne de la variable $Y$, ainsi que l'intervalle de confiance associé pour chacun d'entre eux. Nous pourrons ensuite comparer les résultats obtenus.

Pour ce faire, plusieurs méthodes seront utilisées. En effet, dans un premier temps, nous allons utiliser uniquement les données non manquantes de la variable (cas complets). Puis, nous testerons les différentes méthodes d'imputation suivantes pour les valeurs non renseignées :

-- L'imputation simple par la moyenne.

-- L'imputation simple par régression stochastique.

-- L'imputation multiple par régression stochastique.

-- L'imputation multiple par régression.

-   Analyse des cas complets

Comme évoqué, nous débutons par une analyse des cas complets. Cela consiste à supprimer les observations pour lesquelles la valeur de $Y$ est manquante. C'est la méthode la plus simple. Cependant, elle est déconseillée lorsque le nombre de données manquantes est trop élevé car nous perdrions trop d'informations. De plus, pour les jeux de données où les valeurs manquantes ne sont pas type MCAR, retirer des observations va induire un biais dans l'analyse puisque l'ensemble des observations pour lesquelles des données sont manquantes n'est pas forcément représentatif de l'échantillon initial.

Dans notre cas, nous avons environ $30$% de Na dans nos différents jeux de données. De plus, deux d'entre eux comportent des valeurs manquantes qui ne sont pas de type MCAR. Il ne serait donc pas judicieux de procéder ainsi. Cependant, l'objectif ici étant de comparer plusieurs méthodes, nous allons tout de même le réaliser. Pour cela, nous utilisons la fonction `na.omit`.

```{r, echo = F}
don.ismcar.cc <- na.omit(don.ismcar)
don.ismar.cc <- na.omit(don.ismar)
don.ismnar.cc <- na.omit(don.ismnar)
```

Nous pouvons regarder la dimension de nos nouveaux jeux de données "complets".

```{r, echo = F}
dfMCAR <- data.frame(dim(don.ismcar.cc), row.names = c('Observations', "Variables"))
colnames(dfMCAR) = "MCAR"

dfMAR <- data.frame(dim(don.ismar.cc), row.names = c('Observations', "Variables"))
colnames(dfMAR) = "MAR"

dfMNAR <- data.frame(dim(don.ismnar.cc), row.names = c('Observations', "Variables"))
colnames(dfMNAR) = "MNAR"

kable(data.frame(cbind(dfMCAR,dfMAR,dfMNAR)),position="H", caption = "Dimension du jeu de données complet")
```

La dimension initiale était de $100$ observations. Après suppression, nous en gardons nettement moins. Rappelons que les lignes retirées ne sont pas les mêmes pour chacun des jeux de données car les valeurs manquantes ne se situaient pas au même endroit.

Nous pouvons à présent estimer la moyenne de $Y$. Pour cela, nous réalisons une régression simple sur la variable, grâce à la fonction `lm` de la librairie `stats`, et nous récupérons l'intercept. Les moyennes obtenues sont présentes dans le tableau suivant :

```{r, echo = F}
lm.mcar.cc <- lm(don.ismcar.cc$Y~1)
lm.mar.cc <- lm(don.ismar.cc$Y~1)
lm.mnar.cc <- lm(don.ismnar.cc$Y~1)

mean.mcar.cc <- lm.mcar.cc$coefficients
mean.mar.cc <- lm.mar.cc$coefficients
mean.mnar.cc <- lm.mnar.cc$coefficients

kable(data.frame(mean.mcar.cc,mean.mar.cc,mean.mnar.cc), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y des cas complets")
```

On remarque que les moyennes sont très différentes selon les jeux de données. La première est celle qui se rapporche le plus de $0$, la vraie valeur.

Regardons maintenant les intervalles de confiance associés :

```{r, echo = F}
IC <- rbind(data.frame(confint(lm.mcar.cc)), data.frame(confint(lm.mar.cc)), data.frame(confint(lm.mnar.cc)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y des cas complets")
```

Nous remarquons ici que les deux premiers jeux de données, correspondants aux types MCAR et MAR, ont de "bons" résultats. En effet, contrairement au troisième, le $0$ est présent dans l'intervalle. Cependant, le premier semble plus centré autour de la vraie valeur alors que le deuxième la contient de peu. Cela confirme ce que nous avions évoqué précédemment sur le biais que provoque cette méthode lorsque les données manquantes ne sont pas de type MCAR.

-   Imputation simple par la moyenne

Pour éviter de supprimer les observations où se trouvent des données manquantes, on peut décider d'imputer des valeurs à ces endroits. Plusieurs méthodes existent pour déterminer la valeur à indiquer. La première que nous testons consiste à imputer le Na par la moyenne de la variable. Toutes les données manquantes seront donc "remplacées" par la même valeur.

Pour l'estimation de la moyenne de la variable ensuite, nous obtenons forcément les mêmes résultats qu'auparavant :

```{r, echo = F}
don.ismcar.im <- na.aggregate(don.ismcar, FUN = mean)
don.ismar.im <- na.aggregate(don.ismar, FUN = mean)
don.ismnar.im <- na.aggregate(don.ismnar, FUN = mean)

lm.mcar.im <- lm(don.ismcar.im$Y~1)
lm.mar.im <- lm(don.ismar.im$Y~1)
lm.mnar.im <- lm(don.ismnar.im$Y~1)

mean.mcar.im <- lm.mcar.im$coefficients
mean.mar.im <- lm.mar.im$coefficients
mean.mnar.im <- lm.mnar.im$coefficients

kable(data.frame(mean.mcar.im,mean.mar.im,mean.mnar.im), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après imputation simple par la moyenne")
```

Regardons ce que donnent les intervalles de confiance associés :

```{r, echo = F}
IC <- rbind(data.frame(confint(lm.mcar.im)), data.frame(confint(lm.mar.im)), data.frame(confint(lm.mnar.im)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y après imputation simple par la moyenne")
```

Les intervalles de confiance ont changés. Il y a maintenant uniquement dans le premier cas (MCAR) que $0$ se situe dans l'intervalle. En effet, pour les deux autres mécansimes, l'intervalle s'est décalé dans les négatifs. De plus, leur longueur a diminué. Ainsi, ils offrent plus de précision dans leur estimation.

-   Imputation simple par régression stochastique

La deuxième méthode d'imputation simple que nous testons est l'imputation par régression stochastique. Celle-ci consiste à "remplacer" une donnée non observée par une valeur prédite obtenue en régressant la variable manquante sur d'autres variables, à laquelle on ajoute une valeur résiduelle aléatoire. Cela permet de préserver les relations entre les variables, mais aussi d'avoir l'avantage d'une composante aléatoire.

Concrètement, pour réaliser cette méthode, il nous faut tirer aléatoirement une valeur autour de celle prédite par le modèle de régression. Pour cela, nous utilisons la fonction `mice` avec la méthode `norm.nob`.

Nous visualisons ensuite les données pour s'assurer que la distribution des données imputées est globalement la même que celle des données initiales.

```{r, echo = F, fig.height=4,fig.height=4, fig.cap="Imputation simple par régression stochastique MCAR"}
set.seed(122)
don.ismcar.irs <- complete(mice(don.ismcar, method = "norm.nob", m = 1, print = FALSE))
don.ismar.irs <- complete(mice(don.ismar, method = "norm.nob", m = 1, print = FALSE))
don.ismnar.irs <- complete(mice(don.ismnar, method = "norm.nob", m = 1, print = FALSE))

plot(don.ismcar.irs$X[!is.na(don.ismcar$Y)], don.ismcar.irs$Y[!is.na(don.ismcar$Y)],  
     xlim = c(-3, 3), ylim = c(- 5, 5),
     main = "MCAR",
     xlab = "X", ylab = "Y")
points(don.ismcar.irs$X[is.na(don.ismcar$Y)], don.ismcar.irs$Y[is.na(don.ismcar$Y)], col = "red")
abline(lm(Y ~ X, don.ismcar), col = "blue", lwd = 1.5)
legend("topleft",                                        
       c("Valeurs observées", "Valeurs imputées", "Régression Y ~ X"),
       pch = c(1, 1, NA),
       lty = c(NA, NA, 1),
       col = c("black", "red", "blue"))
```

```{r, echo = F, fig.height=4,fig.cap="Imputation simple par régression stochastique MAR"}
plot(don.ismar.irs$X[!is.na(don.ismar$Y)], don.ismar.irs$Y[!is.na(don.ismar$Y)],  
     xlim = c(-3, 3), ylim = c(- 5, 5),
     main = "MAR",
     xlab = "X", ylab = "Y")
points(don.ismar.irs$X[is.na(don.ismar$Y)], don.ismar.irs$Y[is.na(don.ismar$Y)], col = "red")
abline(lm(Y ~ X, don.ismar), col = "blue", lwd = 1.5)
legend("topleft",                                        
       c("Valeurs observées", "Valeurs imputées", "Régression Y ~ X"),
       pch = c(1, 1, NA),
       lty = c(NA, NA, 1),
       col = c("black", "red", "blue"))
```

```{r, echo = F, fig.height=4,fig.cap="Imputation simple par régression stochastique MNAR"}

plot(don.ismnar.irs$X[!is.na(don.ismnar$Y)], don.ismnar.irs$Y[!is.na(don.ismnar$Y)],  
     xlim = c(-3, 3), ylim = c(- 5, 5),
     main = "MNAR",
     xlab = "X", ylab = "Y")
points(don.ismnar.irs$X[is.na(don.ismnar$Y)], don.ismnar.irs$Y[is.na(don.ismnar$Y)], col = "red")
abline(lm(Y ~ X, don.ismnar), col = "blue", lwd = 1.5)
legend("topleft",                                        
       c("Valeurs observées", "Valeurs imputées", "Régression Y ~ X"),
       pch = c(1, 1, NA),
       lty = c(NA, NA, 1),
       col = c("black", "red", "blue"))
```

Les différentes imputations semblent réalistes car elles suivent une distribution semblable à celles des données déjà existantes.

Nous estimons ensuite les moyennes et obtenons les résultats suivants :

```{r, echo = F}
set.seed(122)
lm.mcar.irs <- lm(don.ismcar.irs$Y~1)
lm.mar.irs <- lm(don.ismar.irs$Y~1)
lm.mnar.irs <- lm(don.ismnar.irs$Y~1)

mean.mcar.irs <- lm.mcar.irs$coefficients
mean.mar.irs <- lm.mar.irs$coefficients
mean.mnar.irs <- lm.mnar.irs$coefficients

kable(data.frame(mean.mcar.irs,mean.mar.irs,mean.mnar.irs), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après imputation simple par régression stochastique")
```

Cette fois-ci, c'est le deuxième cas (MAR) qui obtient le meilleur résultat car la moyenne obtenue est très proche de celle attendue.

Nous pouvons regarder ce que donnent les intervalles de confiance associés :

```{r, echo = F}
IC <- rbind(data.frame(confint(lm.mcar.irs)), data.frame(confint(lm.mar.irs)), data.frame(confint(lm.mnar.irs)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y après imputation simple par régression stochastique")
```

Les intervalles de confiance obtenus avec cette méthode d'imputation semblent meilleurs que ceux des méthodes précédemment testées. En effet, les deux premiers contiennent $0$ et le troisième s'en rapproche. Cependant, leur longueur est plus grande que pour l'imputation avec la moyenne. Ils sont donc moins précis.

-   Imputation multiple par régression stochastique

La méthode précédente ayant des résultats encourageants, nous souhaitons poursuivre son étude. En effet, une valeur unique ne pouvant pas refléter l'incertitude sur la prédiction, nous allons tester de renouveller vingt fois l'imputation avant de combiner les résultats. C'est ce qu'on appelle l'imputation multiple. Pour la réaliser, nous utilisons toujours la fonction `mice` mais en précisant l'argument `m=20`.

Nous obtenons alors $20$ jeu de données imputés. Regardons les valeurs pour chacun d'eux, dans les trois cas que nous étudions :

```{r, echo = F, fig.height=4, fig.cap= "Imputation multiple par régression stochastique"}
set.seed(123)
don.ismcar.irs20 <- mice(don.ismcar, method = "norm.nob", m = 20, print = FALSE)
don.ismar.irs20 <- mice(don.ismar, method = "norm.nob", m = 20, print = FALSE)
don.ismnar.irs20 <- mice(don.ismnar, method = "norm.nob", m = 20, print = FALSE)

stripplot(don.ismcar.irs20, pch = 20, cex = 1.2, main = "MCAR")
```

```{r, echo = F, fig.height=4,fig.cap= "Imputation multiple par régression stochastique MCAR"}
stripplot(don.ismar.irs20, pch = 20, cex = 1.2, main = "MAR")
```
```{r, echo = F, fig.height=4,fig.cap= "Imputation multiple par régression stochastique MNAR"}
stripplot(don.ismnar.irs20, pch = 20, cex = 1.2, main = "MNAR")
```

Les données imputées se trouvant en rouge, nous remarquons que chaque imputation est différente, mais respecte la distribution des données initiales.

Il faut maintenant analyser et combiner les données en utilisant les fonctions `with` et `pool` et on récupère la moyenne estimée.

```{r, echo  = F}
set.seed(123)

fit_mcar.irs20 <- with(don.ismcar.irs20, lm(Y~1))
fitpool_mcar.irs20 <- pool(fit_mcar.irs20)
mean.mcar.irs20 <- fitpool_mcar.irs20$pooled[3]

fit_mar.irs20 <- with(don.ismar.irs20, lm(Y~1))
fitpool_mar.irs20 <- pool(fit_mar.irs20)
mean.mar.irs20 <- fitpool_mar.irs20$pooled[3]

fit_mnar.irs20 <- with(don.ismnar.irs20, lm(Y~1))
fitpool_mnar.irs20 <- pool(fit_mnar.irs20)
mean.mnar.irs20 <- fitpool_mnar.irs20$pooled[3]

kable(data.frame(mean.mcar.irs20,mean.mar.irs20,mean.mnar.irs20), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après combinaison des 20 imputations par régression stochastique")
```

Une nouvelle fois, le mécanisme MAR obtient le meilleur résultat, même si il s'éloigne légèrement plus de $0$. Cependant, l'incertitude de l'imputation des données manquantes étant prise en compte, on peut considérer que ce sont des résultats plus "justes".

Regardons les intervalles de confiance associés :

```{r, echo=F}
inf_mcar.irs20 <- summary(fitpool_mcar.irs20, conf.int = TRUE)[7]
sup_mcar.irs20 <- summary(fitpool_mcar.irs20, conf.int = TRUE)[8]


inf_mar.irs20 <- summary(fitpool_mar.irs20, conf.int = TRUE)[7]
sup_mar.irs20 <- summary(fitpool_mar.irs20, conf.int = TRUE)[8]

inf_mnar.irs20 <- summary(fitpool_mnar.irs20, conf.int = TRUE)[7]
sup_mnar.irs20 <- summary(fitpool_mnar.irs20, conf.int = TRUE)[8]

IC <- rbind(data.frame(c(inf_mcar.irs20, sup_mcar.irs20)), data.frame(c(inf_mar.irs20, sup_mar.irs20)), data.frame(c(inf_mnar.irs20, sup_mnar.irs20)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y après 20 imputations par régression stochastique")
```

Les conclusions sont les mêmes que pour les moyennes. En effet, les intervalles sont un peu plus grands mais sont sensiblement répartis de la même façon par rapport à $0$ que pour l'imputation simple.

-   Imputation multiple par régression

Enfin, nous allons tester de réaliser une nouvelle fois une imputation multiple. En effet, nous réalisons vingt imputations, mais cette fois-ci par régression linéaire. Nous le précisons dans le code en indiquant `method=norm` dans la fonction `mice`.

Nous obtenons les distributions suivantes :

```{r, echo = F, fig.height=4, fig.cap= "Imputation multiple par régression MCAR"}
set.seed(123)
don.ismcar.ir20 <- mice(don.ismcar, method = "norm", m = 20, print = FALSE)
don.ismar.ir20 <- mice(don.ismar, method = "norm", m = 20, print = FALSE)
don.ismnar.ir20 <- mice(don.ismnar, method = "norm", m = 20, print = FALSE)

stripplot(don.ismcar.ir20, pch = 20, cex = 1.2, main = "MCAR")
```

```{r, echo = F, fig.height=4, fig.cap= "Imputation multiple par régression MAR"}
stripplot(don.ismar.ir20, pch = 20, cex = 1.2, main = "MAR")
```

```{r, echo = F, fig.height=4, fig.cap= "Imputation multiple par régression MNAR"}
stripplot(don.ismnar.ir20, pch = 20, cex = 1.2, main = "MNAR")
```

Nous calculons ensuite les moyennes et nous obtenons les résultats suivants :

```{r, echo = F}
set.seed(123)
fit_mcar.ir20 <- with(don.ismcar.ir20, lm(Y~1))
fitpool_mcar.ir20 <- pool(fit_mcar.ir20)
mean.mcar.ir20 <- fitpool_mcar.ir20$pooled[3]

fit_mar.ir20 <- with(don.ismar.ir20, lm(Y~1))
fitpool_mar.ir20 <- pool(fit_mar.ir20)
mean.mar.ir20 <- fitpool_mar.ir20$pooled[3]

fit_mnar.ir20 <- with(don.ismnar.ir20, lm(Y~1))
fitpool_mnar.ir20 <- pool(fit_mnar.ir20)
mean.mnar.ir20 <- fitpool_mnar.ir20$pooled[3]

kable(data.frame(mean.mcar.ir20,mean.mar.ir20,mean.mnar.ir20), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après combinaison des 20 imputations par régression")
```

Le deuxième jeu de données obtient encore de bons résultats. L'imputation par régression semble donc mieux adaptée pour le mécanisme MAR.

Regardons si cela se confirme pour les intervalles de confiance associés aux moyennes :

```{r}
inf_mcar.ir20 <- summary(fitpool_mcar.ir20, conf.int = TRUE)[7]
sup_mcar.ir20 <- summary(fitpool_mcar.ir20, conf.int = TRUE)[8]

inf_mar.ir20 <- summary(fitpool_mar.ir20, conf.int = TRUE)[7]
sup_mar.ir20 <- summary(fitpool_mar.ir20, conf.int = TRUE)[8]

inf_mnar.ir20 <- summary(fitpool_mnar.ir20, conf.int = TRUE)[7]
sup_mnar.ir20 <- summary(fitpool_mnar.ir20, conf.int = TRUE)[8]

IC <- rbind(data.frame(c(inf_mcar.ir20, sup_mcar.ir20)), data.frame(c(inf_mar.ir20, sup_mar.ir20)), data.frame(c(inf_mnar.ir20, sup_mnar.ir20)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y après 20 imputations par régression")
```

Nous observons que la vraie valeur est située dans les deux premiers intervalles uniquement. De plus, le deuxième semble être plus centré autour de $0$.

### Comparaison des stratégies

Afin de comparer chacune des méthodes précédentes, nous allons recommencer toutes les procédures $200$ fois. Cela nous permettra d'estimer le biais commis sur l'estimation de la moyenne ainsi que la longueur moyenne et le taux de couverture de l'intervalle de confiance.

```{r, echo = F}
set.seed(123)
mean.mcar.cc <- c(1:200)
mean.mar.cc <- c(1:200)
mean.mnar.cc <- c(1:200)

mean.mcar.im <- c(1:200)
mean.mar.im <- c(1:200)
mean.mnar.im <- c(1:200)

mean.mcar.irs <- c(1:200)
mean.mar.irs <- c(1:200)
mean.mnar.irs <- c(1:200)

mean.mcar.irs20 <- c(1:200)
mean.mar.irs20 <- c(1:200)
mean.mnar.irs20 <- c(1:200)

mean.mcar.ir20 <- c(1:200)
mean.mar.ir20 <- c(1:200)
mean.mnar.ir20 <- c(1:200)

IC.mcar.cc <- matrix(ncol = 2, nrow = 200)
IC.mar.cc <- matrix(ncol = 2, nrow = 200)
IC.mnar.cc <- matrix(ncol = 2, nrow = 200)

IC.mcar.im <- matrix(ncol = 2, nrow = 200)
IC.mar.im <- matrix(ncol = 2, nrow = 200)
IC.mnar.im <- matrix(ncol = 2, nrow = 200)

IC.mcar.irs <- matrix(ncol = 2, nrow = 200)
IC.mar.irs <- matrix(ncol = 2, nrow = 200)
IC.mnar.irs <- matrix(ncol = 2, nrow = 200)

IC.mcar.irs20 <- matrix(ncol = 2, nrow = 200)
IC.mar.irs20 <- matrix(ncol = 2, nrow = 200)
IC.mnar.irs20 <- matrix(ncol = 2, nrow = 200)

IC.mcar.ir20 <- matrix(ncol = 2, nrow = 200)
IC.mar.ir20 <- matrix(ncol = 2, nrow = 200)
IC.mnar.ir20 <- matrix(ncol = 2, nrow = 200)

for (i in 1:200){
  don <- rmvnorm(n=n, mean = mu, sigma = matcov)
  colnames(don) <- c("X", "Y") 
  
  #MCAR
  don.ismcar = don
  ismcar <- sample(c(T,F), size = n, prob = c(0.35, 0.65), replace = TRUE)
  don.ismcar[ismcar, "Y"] <- NA
  don.ismcar <- as.data.frame(don.ismcar)
  
  #MAR
  don.ismar <- don
  ismar <- sapply(don.ismar[,"X"], FUN=function(xx){
    prob <- pnorm(1.2*xx-.5) 
    res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
    return (res)
  })
  don.ismar[ismar,"Y"] <- NA 
  don.ismar <- as.data.frame(don.ismar)
  
  #MNAR
  don.ismnar <- don
  ismnar <- sapply(don.ismnar[,"Y"], FUN=function(xx){
    prob <- pnorm(1.2*xx-.5) 
    res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
    return (res)
  })
  don.ismnar[ismnar,"Y"] <- NA 
  don.ismnar <- as.data.frame(don.ismnar)
  
  #Cas complet
  don.ismcar.cc <- na.omit(don.ismcar)
  don.ismar.cc <- na.omit(don.ismar)
  don.ismnar.cc <- na.omit(don.ismnar)
  
  lm.mcar.cc <- lm(don.ismcar.cc$Y~1)
  lm.mar.cc <- lm(don.ismar.cc$Y~1)
  lm.mnar.cc <- lm(don.ismnar.cc$Y~1)

  mean.mcar.cc[i] <- lm.mcar.cc$coefficients
  mean.mar.cc[i] <- lm.mar.cc$coefficients
  mean.mnar.cc[i] <- lm.mnar.cc$coefficients
  
  IC.mcar.cc[i,] <- confint(lm.mcar.cc)
  IC.mar.cc[i,] <- confint(lm.mar.cc)
  IC.mnar.cc[i,] <- confint(lm.mnar.cc)
  
  #Imputation par la moyenne
  don.ismcar.im <- na.aggregate(don.ismcar, FUN = mean)
  don.ismar.im <- na.aggregate(don.ismar, FUN = mean)
  don.ismnar.im <- na.aggregate(don.ismnar, FUN = mean)

  lm.mcar.im <- lm(don.ismcar.im$Y~1)
  lm.mar.im <- lm(don.ismar.im$Y~1)
  lm.mnar.im <- lm(don.ismnar.im$Y~1)

  mean.mcar.im[i] <- lm.mcar.im$coefficients
  mean.mar.im[i] <- lm.mar.im$coefficients
  mean.mnar.im[i] <- lm.mnar.im$coefficients
  
  IC.mcar.im[i,] <- confint(lm.mcar.im)
  IC.mar.im[i,] <- confint(lm.mar.im)
  IC.mnar.im[i,] <- confint(lm.mnar.im)
  
  #Imputation par régression stochastique
  don.ismcar.irs <- complete(mice(don.ismcar, method = "norm.nob", m = 1, print = FALSE))
  don.ismar.irs <- complete(mice(don.ismar, method = "norm.nob", m = 1, print = FALSE))
  don.ismnar.irs <- complete(mice(don.ismnar, method = "norm.nob", m = 1, print = FALSE))
  
  lm.mcar.irs <- lm(don.ismcar.irs$Y~1)
  lm.mar.irs <- lm(don.ismar.irs$Y~1)
  lm.mnar.irs <- lm(don.ismnar.irs$Y~1)

  mean.mcar.irs[i] <- lm.mcar.irs$coefficients
  mean.mar.irs[i] <- lm.mar.irs$coefficients
  mean.mnar.irs[i] <- lm.mnar.irs$coefficients
  
  IC.mcar.irs[i,] <- confint(lm.mcar.irs)
  IC.mar.irs[i,] <- confint(lm.mar.irs)
  IC.mnar.irs[i,] <- confint(lm.mnar.irs)
  
  #20 imputations par régression stochastique
  don.ismcar.irs20 <- mice(don.ismcar, method = "norm.nob", m = 20, print = FALSE)
  don.ismar.irs20 <- mice(don.ismar, method = "norm.nob", m = 20, print = FALSE)
  don.ismnar.irs20 <- mice(don.ismnar, method = "norm.nob", m = 20, print = FALSE)
  
  fit_mcar.irs20 <- with(don.ismcar.irs20, lm(Y~1))
  fitpool_mcar.irs20 <- pool(fit_mcar.irs20)

  fit_mar.irs20 <- with(don.ismar.irs20, lm(Y~1))
  fitpool_mar.irs20 <- pool(fit_mar.irs20)

  fit_mnar.irs20 <- with(don.ismnar.irs20, lm(Y~1))
  fitpool_mnar.irs20 <- pool(fit_mnar.irs20)
  
  mean.mcar.irs20[i] <- summary(fitpool_mcar.irs20, conf.int = TRUE)[2][1,1]
  mean.mar.irs20[i] <- summary(fitpool_mar.irs20, conf.int = TRUE)[2][1,1]
  mean.mnar.irs20[i] <- summary(fitpool_mnar.irs20, conf.int = TRUE)[2][1,1]
  
  inf_mcar.irs20 <- summary(fitpool_mcar.irs20, conf.int = TRUE)[7]
  sup_mcar.irs20 <- summary(fitpool_mcar.irs20, conf.int = TRUE)[8]

  inf_mar.irs20 <- summary(fitpool_mar.irs20, conf.int = TRUE)[7]
  sup_mar.irs20 <- summary(fitpool_mar.irs20, conf.int = TRUE)[8]

  inf_mnar.irs20 <- summary(fitpool_mnar.irs20, conf.int = TRUE)[7]
  sup_mnar.irs20 <- summary(fitpool_mnar.irs20, conf.int = TRUE)[8]
  
  IC.mcar.irs20[i,] <- matrix(c(inf_mcar.irs20[1,1], sup_mcar.irs20[1,1]), ncol = 2)
  IC.mar.irs20[i,] <- matrix(c(inf_mar.irs20[1,1], sup_mar.irs20[1,1]), ncol = 2)
  IC.mnar.irs20[i,] <- matrix(c(inf_mnar.irs20[1,1], sup_mnar.irs20[1,1]), ncol = 2)
  
  #20 imputations par régression
  don.ismcar.ir20 <- mice(don.ismcar, method = "norm", m = 20, print = FALSE)
  don.ismar.ir20 <- mice(don.ismar, method = "norm", m = 20, print = FALSE)
  don.ismnar.ir20 <- mice(don.ismnar, method = "norm", m = 20, print = FALSE)
  
  fit_mcar.ir20 <- with(don.ismcar.ir20, lm(Y~1))
  fitpool_mcar.ir20 <- pool(fit_mcar.ir20)

  fit_mar.ir20 <- with(don.ismar.ir20, lm(Y~1))
  fitpool_mar.ir20 <- pool(fit_mar.ir20)

  fit_mnar.ir20 <- with(don.ismnar.ir20, lm(Y~1))
  fitpool_mnar.ir20 <- pool(fit_mnar.ir20)
  
  mean.mcar.ir20[i] <- summary(fitpool_mcar.ir20, conf.int = TRUE)[2][1,1]
  mean.mar.ir20[i] <- summary(fitpool_mar.ir20, conf.int = TRUE)[2][1,1]
  mean.mnar.ir20[i] <- summary(fitpool_mnar.ir20, conf.int = TRUE)[2][1,1]
  
  inf_mcar.ir20 <- summary(fitpool_mcar.ir20, conf.int = TRUE)[7]
  sup_mcar.ir20 <- summary(fitpool_mcar.ir20, conf.int = TRUE)[8]

  inf_mar.ir20 <- summary(fitpool_mar.ir20, conf.int = TRUE)[7]
  sup_mar.ir20 <- summary(fitpool_mar.ir20, conf.int = TRUE)[8]

  inf_mnar.ir20 <- summary(fitpool_mnar.ir20, conf.int = TRUE)[7]
  sup_mnar.ir20 <- summary(fitpool_mnar.ir20, conf.int = TRUE)[8]
  
  IC.mcar.ir20[i,] <- matrix(c(inf_mcar.ir20[1,1], sup_mcar.ir20[1,1]), ncol = 2)
  IC.mar.ir20[i,] <- matrix(c(inf_mar.ir20[1,1], sup_mar.ir20[1,1]), ncol = 2)
  IC.mnar.ir20[i,] <- matrix(c(inf_mnar.ir20[1,1], sup_mnar.ir20[1,1]), ncol = 2)
}
```

-   Biais commis sur l'estimation de la moyenne

Nous savons que le biais s'écrit de la forme $Biais(\hat{\theta})=\mathbb{E}(\hat{\theta}) - \theta$, où $\hat{\theta}$ est l'estimateur de $\theta$. Ici, on veut estimer la moyenne dont la vraie valeur est $0$. Le biais vaut donc l'espérance de l'estimateur. Ainsi, analyser le biais ici équivaut à regarder la moyenne estimée sur nos échantillons comme nous l'avons fait auparavant (en récupérant l'estimation après régression simple).

Commençons donc par regarder l'évolution de la moyenne sur ces $200$ imputations pour chacune des méthodes :

```{r, echo = F, fig.height=4.5,fig.cap= "Evolution de la moyenne de Y pour les cas complets"}
mat.cc <- matrix(c(mean.mcar.cc,mean.mar.cc,mean.mnar.cc), ncol = 3)
mat.im <- matrix(c(mean.mcar.im,mean.mar.im,mean.mnar.im), ncol = 3)
mat.irs <- matrix(c(mean.mcar.irs,mean.mar.irs,mean.mnar.irs), ncol = 3)
mat.irs20 <- matrix(c(mean.mcar.irs20,mean.mar.irs20,mean.mnar.irs20), ncol = 3)
mat.ir20 <- matrix(c(mean.mcar.ir20,mean.mar.ir20,mean.mnar.ir20), ncol = 3)

matplot(mat.cc, type = 'l', lty = 1, col = c("red","black", "blue"), ylab = "Moyenne de Y", main = "Cas complets")
legend(5, 0.4, c("MCAR", " MAR", "MNAR"), pch = 'l', col = c("red","black", "blue"))
```

```{r, echo = F, fig.height=4.5,fig.cap= "Evolution de la moyenne de Y après imputation simple par la moyenne"}
matplot(mat.im, type = 'l', lty = 1, col = c("red","black", "blue"), ylab = "Moyenne de Y", main = "Imputation simple par la moyenne")
legend(5, 0.4, c("MCAR", " MAR", "MNAR"), pch = 'l', col = c("red","black", "blue"))
```

```{r, echo = F, fig.height=4.5,fig.cap= "Evolution de la moyenne de Y après imputation simple par régression stochastique"}
matplot(mat.irs, type = 'l', lty = 1, col = c("red","black", "blue"), ylab = "Moyenne de Y", main = "Imputation simple par régression stochastique")
legend(5, 0.4, c("MCAR", " MAR", "MNAR"), pch = 'l', col = c("red","black", "blue"))
```
```{r, echo = F, fig.height=4.5,fig.cap= "Evolution de la moyenne de Y après imputation multiple par régression stochastique"}
matplot(mat.irs20, type = 'l', lty = 1, col = c("red","black", "blue"), ylab = "Moyenne de Y", main = "Imputation multiple par régression stochastique")
legend(5, 0.4, c("MCAR", " MAR", "MNAR"), pch = 'l', col = c("red","black", "blue"))
```
```{r, echo = F, fig.height=4.5,fig.cap= "Evolution de la moyenne de Y après imputation multiple par régression"}
matplot(mat.ir20, type = 'l', lty = 1, col = c("red","black", "blue"), ylab = "Moyenne de Y", main = "Imputation multiple par régression")
legend(5, 0.4, c("MCAR", " MAR", "MNAR"), pch = 'l', col = c("red","black", "blue"))
```

Tout d'abord, nous remarquons que les moyennes obtenues avec le mécanisme MCAR sont toujours autour de $0$. Lorsque les impuations sont réalisées par régression, les moyennes du jeu de données associé au mécanisme MAR s'améliorent nettement et se rapprochent de la valeur attendue. Quant au mécansime MNAR, peu importe les méthodes d'imputation utilisées par la suite, les moyennes estimées sont toujours dans les négatifs. Nous confirmons alors la complexité d'analyse des jeux de données ayant des valeurs manquantes de ce type.

-   Longueur moyenne de l'intervalle de confiance

Regardons maintenant comment évolue l'intervalle de confiance associé à la moyenne. Pour ce faire, nous résumons dans le tableau suivant les différentes longueurs d'intervalles obtenues :

```{r, echo = F}
l.mcar.cc <- mean(IC.mcar.cc[,2]-IC.mcar.cc[,1])
l.mar.cc <- mean(IC.mar.cc[,2]-IC.mar.cc[,1])
l.mnar.cc <- mean(IC.mnar.cc[,2]-IC.mnar.cc[,1])

l.mcar.im <- mean(IC.mcar.im[,2]-IC.mcar.im[,1])
l.mar.im <- mean(IC.mar.im[,2]-IC.mar.im[,1])
l.mnar.im <- mean(IC.mnar.im[,2]-IC.mnar.im[,1])

l.mcar.irs <- mean(IC.mcar.irs[,2]-IC.mcar.irs[,1])
l.mar.irs <- mean(IC.mar.irs[,2]-IC.mar.irs[,1])
l.mnar.irs <- mean(IC.mnar.irs[,2]-IC.mnar.irs[,1])

l.mcar.irs20 <- mean(IC.mcar.irs20[,2]-IC.mcar.irs20[,1])
l.mar.irs20 <- mean(IC.mar.irs20[,2]-IC.mar.irs20[,1])
l.mnar.irs20 <- mean(IC.mnar.irs20[,2]-IC.mnar.irs20[,1])

l.mcar.ir20 <- mean(IC.mcar.ir20[,2]-IC.mcar.ir20[,1])
l.mar.ir20 <- mean(IC.mar.ir20[,2]-IC.mar.ir20[,1])
l.mnar.ir20 <- mean(IC.mnar.ir20[,2]-IC.mnar.ir20[,1])

L <- rbind(data.frame('1'= l.mcar.cc, '2' = l.mar.cc, '3' = l.mnar.cc),data.frame('1'= l.mcar.im,'2'= l.mar.im,'3' = l.mnar.im),data.frame('1'= l.mcar.irs,'2'= l.mar.irs,'3' = l.mnar.irs), data.frame('1'= l.mcar.irs20,'2'= l.mar.irs20,'3' = l.mnar.irs20), data.frame('1'= l.mcar.ir20,'2'= l.mar.ir20,'3' = l.mnar.ir20))

colnames(L) <- c("MCAR", "MAR", "MNAR")
rownames(L) <- c("Cas complets", "Imputation simple par la moyenne", "Imputation simple par régression stochastqiue", "Imputation multiple par régression stochastique", "Imputation multiple par régression")

kable(data.frame(L), col.names = c("MCAR", "MAR", "MNAR"), position="H", caption = "Longueur moyenne de l'intervalle de confiance")
```

Nous observons que l'imputation par la moyenne obtient les longueurs d'intervalle les plus faibles, suivie par l'imputation simple par régression stochastique. Cependant, il ne faut pas oublier que dans ces cas, on ne tient pas compte du fait que des valeurs ont été remplacées et on fait comme si c'était de vraies observations. Cela va en général sous-évaluer la variabilité dans les données et peut expliquer des intervalles de confiance plus courts.

Les imputations multiples obtiennent donc des intervalles plus grands mais tentent de mieux reproduire la variabilité des données. Lorsque l'on compare les deux imputations de ce type effectuées, nous remarquons que celle par régression stochastique semble être plus performante car fournit un intervalle plus précis. De plus, les résultats sont également meilleurs que lorsque l'on a étudié uniquement les cas complets.

-   Taux de couverture de l'intervalle de confiance

Enfin, nous allons analyser le taux de couverture des intervalles de confiance. En effet, la valeur attendue de la moyenne étant $0$, nous regardons combien de fois celle-ci est comprise dans l'intervalle obtenu. C'est ce que nous résumons dans le tableau suivant :

```{r, echo=F}
tc.mcar.cc <- sum(sign(IC.mcar.cc[,1]*IC.mcar.cc[,2])<0)
tc.mar.cc <- sum(sign(IC.mar.cc[,1]*IC.mar.cc[,2])<0)
tc.mnar.cc <- sum(sign(IC.mnar.cc[,1]*IC.mnar.cc[,2])<0)

tc.mcar.im <- sum(sign(IC.mcar.im[,1]*IC.mcar.im[,2])<0)
tc.mar.im <- sum(sign(IC.mar.im[,1]*IC.mar.im[,2])<0)
tc.mnar.im <- sum(sign(IC.mnar.im[,1]*IC.mnar.im[,2])<0)

tc.mcar.irs <- sum(sign(IC.mcar.irs[,1]*IC.mcar.irs[,2])<0)
tc.mar.irs <- sum(sign(IC.mar.irs[,1]*IC.mar.irs[,2])<0)
tc.mnar.irs <- sum(sign(IC.mnar.irs[,1]*IC.mnar.irs[,2])<0)

tc.mcar.irs20 <- sum(sign(IC.mcar.irs20[,1]*IC.mcar.irs20[,2])<0)
tc.mar.irs20 <- sum(sign(IC.mar.irs20[,1]*IC.mar.irs20[,2])<0)
tc.mnar.irs20 <- sum(sign(IC.mnar.irs20[,1]*IC.mnar.irs20[,2])<0)

tc.mcar.ir20 <- sum(sign(IC.mcar.ir20[,1]*IC.mcar.ir20[,2])<0)
tc.mar.ir20 <- sum(sign(IC.mar.ir20[,1]*IC.mar.ir20[,2])<0)
tc.mnar.ir20 <- sum(sign(IC.mnar.ir20[,1]*IC.mnar.ir20[,2])<0)

L <- rbind(data.frame('1'= tc.mcar.cc, '2' = tc.mar.cc, '3' = tc.mnar.cc),data.frame('1'= tc.mcar.im,'2'= tc.mar.im,'3' = tc.mnar.im),data.frame('1'= tc.mcar.irs,'2'= tc.mar.irs,'3' = tc.mnar.irs), data.frame('1'= tc.mcar.irs20,'2'= tc.mar.irs20,'3' = tc.mnar.irs20), data.frame('1'= tc.mcar.ir20,'2'= tc.mar.ir20,'3' = tc.mnar.ir20))

colnames(L) <- c("MCAR", "MAR", "MNAR")
rownames(L) <- c("Cas complets", "Imputation par la moyenne", "Imputation simple par régression stochastqiue", "Imputation multiple par régression stochastique", "Imputation multiple par régression")

kable(data.frame(L), col.names = c("MCAR", "MAR", "MNAR"), position="H", caption = "Taux de couverture de l'intervalle de confiance")
```

Premièrement, ce tableau confirme les graphiques analysés plus haut. En effet, nous remarquons de manière évidente que le jeu de données associé au mécanisme MNAR n'a quasiment pas permis d'obtenir des intervalles de confiance contenant la vraie valeur que l'on cherche à estimer (la moyenne). De plus, concernant les mécanismes MCAR et MAR, nous voyons une différence pour les cas complets et l'imputation par la moyenne, puis les résultats sont semblables pour les autres. Notons tout de même que, contrairement à la longeur de l'intervalle, la taux de couverture semble être meilleur pour l'imputation multiple par régression que celle par régression stochastique.

\newpage 

**Conclusion :** Nous avons comparé trois mécanismes de répartition des données manquantes (MCAR, MAR et MNAR) sur un jeu de données où nous avons réalisé une analyse des cas complets, deux imputations simples (par la moyenne et par régression stochastique) et deux imputations multiples (par régression stochastique et régression). Après différentes études, nous en concluons de bons résultats pour le mécanisme MCAR ainsi que pour le mécanisme MAR sur les imputations par régression. Quand aux différentes méthodes d'imputation, les résultats des impuations simples semblaient meilleurs mais avec toutes les limites qu'ils comportent sur la variabilité des données. Ainsi, une imputation multiple est donc conseillée. Nous avons obtenu de meilleurs résultats lorsqu'elle était par régression stochastique pour la longueur de l'intervalle. Cependant, pour le taux de couverture, l'imputation multiple par régression était légèrement plus performante.

\newpage

## Scénarios NA sur le jeu de données `mites`

Nous allons dans un premier temps générer 10% de données manquantes sur notre jeu de données `mites` suivant les 3 catégories utilisées precedement, MCAR, MAR et MNAR. Nous allons utiliser la fonction `ampute` du package `mice`. Pour utiliser cette fonction, nous allons uniquement nous concentrer sur les variables quantitatives de notre jeu de données `mites` et ainsi supprimer les variables qualitatives, `Substrate`, `Shrub` et `Topo` et la variable binaire `pa`.

```{r, include=FALSE}
mites1 <- mites[,-c(7,8,9,2)]
mites1
```

```{r}
str(mites1)
```

L'utilisation de la fonction `ampute` necessite une attention particuliere. En effet, dans l'aide de la fonction nous avons que l'argument `prop` signifie "un scalaire spécifiant la proportion de données manquantes". Cependant, nous avons remarqué que l'utilisation seul de cet argument ne nous donne pas 10% de données manquantes sur notre jeu de données. Dans notre exemple ci dessous nous avons seulement 8 données manquantes. Nous obtenons donc un nouveau jeu de données.

```{r}
amp.mcar1 = ampute(mites1, prop = 0.10, mech = "MCAR")
countNA(amp.mcar1$amp)
```

En cherchant un peu plus loin, nous avons remarqué que l'argument `prop` signifie la proportion de lignes incompletes dans notre jeu de données. Nous avons 70 observations pour 5 variables soit 350 données (\~350 cellules). Nous avons donc pensé qu'il etait préferable de générer 10% de données manquantes sur l'ensemble de nos données, soit 10% de 350 données ce qui equivaut à +/- 35 données manquantes. Pour cela, nous devons utiliser l'argument `bycases` comme `FALSE`. En effet, l'argument `bycases` étant `FALSE` signifie que la proportion de données manquantes est définie en termes de cellules et non en terme de lignes, qui est la valeur par défault. Pour spécifier les 3 différents méchanismes MCAR, MAR, MNAR nous utilisons l'argument `mech`.

-   Mécanisme **MCAR** (**M**issing **C**ompletely **A**t **R**andom)

```{r}
amp.mcar = ampute(mites1, prop = 0.10, mech = "MCAR", bycases = FALSE)
countNA(amp.mcar$amp)
```

-   Mécanisme **MAR** (**M**issing **A**t **R**andom)

```{r}
amp.mar = ampute(mites1, prop = 0.10, mech = "MAR", bycases = FALSE)
countNA(amp.mar$amp)
```

-   Mécanisme **MNAR** (**M**issing **N**ot **A**t **R**andom)

```{r}
amp.mnar = ampute(mites1, prop = 0.10, mech = "MNAR", bycases = FALSE)
countNA(amp.mnar$amp)
```

```{r, echo = F}
a <- sum(is.na(amp.mcar$amp))/(nrow(amp.mcar$amp)*ncol(amp.mcar$amp))
b <- sum(is.na(amp.mar$amp))/(nrow(amp.mar$amp)*ncol(amp.mar$amp))
c <- sum(is.na(amp.mnar$amp))/(nrow(amp.mnar$amp)*ncol(amp.mnar$amp))

kable(data.frame(a, b, c), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Proportion de données manquantes")

```
Nous avons donc une proportion de ~10% de données manquantes générées.

### Analyse exploratoire

Nous allons maintenant explorer et analyser les 3 dispositifs de données manquantes créés précedemment. Dans un premier temps, nous ferons une analyse univariée, puis une analyse bivariée et pour finir nous ferons une analyse muldimentionnelle. 

**Analyse univariée**

Nous allons utiliser la fonction `aggr` du package `VIM` pour l'analyse univariée.

```{r, echo = F, fig.cap = MCAR}
aggr(amp.mcar$amp, col=c('navyblue','red'),
                  numbers=TRUE,
                  sortVars=FALSE,
                  labels=names(amp.mcar$amp),
                  cex.axis=.7, gap=3,
                  ylab=c("Proportion de données manquantes","Combinaisons"))
```

Les colonnes représentent les variables, les lignes représentent les types de "situations" des données manquantes générées dans le jeu de données. La ligne entière bleue représente toutes les lignes dans le data frame qui ne comportent pas de données manquantes, soit 50% des lignes dans notre cas. Une case rouge représente toutes les lignes dans le data frame qui sont manquantes pour cette variable. Dans notre cas nous avons par exemple, 11,4% de lignes qui contiennent des données manquantes pour la variable "WatrCont".

```{r, echo = F, fig.cap = MAR}
aggr(amp.mar$amp, col=c('navyblue','red'),
                  numbers=TRUE,
                  sortVars=FALSE,
                  labels=names(amp.mar$amp),
                  cex.axis=.7, gap=3,
                  ylab=c("Proportion de données manquantes","Combinaisons"))
```

On observe que les variables `Galumna` et `totalabund` ont le plus de données manquantes, soit plus de 12% de données manquantes chacunes en comparaison avec la variable `SubsDens` qui a seulement moins de 6% de données manquantes. L'analyse bivariée sera donc plus intéressante dans ce cas. 

```{r, echo = F, fig.cap = MNAR}
aggr(amp.mnar$amp, col=c('navyblue','red'),
                  numbers=TRUE,
                  sortVars=FALSE,
                  labels=names(amp.mnar$amp),
                  cex.axis=.7, gap=3,
                  ylab=c("Proportion de données manquantes","Combinaisons"))
```

Dans ce dispositif, MNAR, on remarque que la proportion de données manquantes pour chaque variable est assez similaire. Nous avons pour chaque variable, une proportion de + ou - 10% de données manquantes. 

**Analyse bivariée**

Pour l'analyse bivariée, nous allons utiliser la fonction `CramerV` du package `DescTools`.
Le test de Cramer est utilisé pour mesurer l'intensité des relations entre les variables. Nous avons ainsi que plus le V de Cramer se rapproche de 1, plus l'intensité de la relation est forte. Nous utilisons la fonction `table` dans un premier temps pour créer un tableau de contingence entre deux variables que nous avons selectionnée. Il est important de spécifier l'argument `useNA` pour inclure les données manquantes dans notre tableau.

Nous avons vu que dans le dispositif MCAR, les variables `SubsDens` et `prop` sont les deux variables ayant le plus de données manquantes. Nous allons donc voir si ces 2 variables ont une forte relation. Nous allons, de plus, tester sur différentes paires de variables.

```{r, echo = F}
tab <- table(amp.mcar$amp$SubsDens, amp.mcar$amp$prop, useNA= "always")
c1 <- CramerV(tab)
tab1 <- table(amp.mcar$amp$prop, amp.mcar$amp$Galumna, useNA= "always")
c2 <- CramerV(tab1)
tab2 <- table(amp.mcar$amp$SubsDens, amp.mcar$amp$Galumna, useNA= "always")
c3 <- CramerV(tab2)
tab3 <- table(amp.mcar$amp$WatrCont, amp.mcar$amp$totalabund, useNA= "always")
c4 <- CramerV(tab2)

kable(data.frame(c1, c2, c3, c4), col.names = c("SubsDens/Prop", "Galumna/Prop", "Galumna/SubsDens", "WatrCont/totalabund"), row.names = FALSE, position="H", caption = "V de Cramer (MCAR)")
```

```{r, echo = F}
tab <- table(amp.mar$amp$SubsDens, amp.mar$amp$prop, useNA= "always")
c1 <- CramerV(tab)
tab1 <- table(amp.mar$amp$prop, amp.mar$amp$Galumna, useNA= "always")
c2 <- CramerV(tab1)
tab2 <- table(amp.mar$amp$WatrCont, amp.mar$amp$Galumna, useNA= "always")
c3 <- CramerV(tab2)
tab3 <- table(amp.mar$amp$SubsDens, amp.mar$amp$totalabund, useNA= "always")
c4 <- CramerV(tab3)

kable(data.frame(c1, c2, c3, c4), col.names = c("SubsDens/Prop", "Galumna/Prop", "Galumna/WatrCont", "SubsDens/totalabund"), row.names = FALSE, position="H", caption = "V de Cramer (MAR)")
```

```{r, echo = F}
tab <- table(amp.mnar$amp$SubsDens, amp.mnar$amp$prop, useNA= "always")
c1 <- CramerV(tab)
tab1 <- table(amp.mnar$amp$prop, amp.mnar$amp$Galumna, useNA= "always")
c2 <- CramerV(tab1)
tab2 <- table(amp.mnar$amp$WatrCont, amp.mnar$amp$Galumna, useNA= "always")
c3 <- CramerV(tab2)
tab3 <- table(amp.mnar$amp$SubsDens, amp.mnar$amp$totalabund, useNA= "always")
c4 <- CramerV(tab3)

kable(data.frame(c1, c2, c3, c4), col.names = c("SubsDens/Prop", "Galumna/Prop", "Galumna/WatrCont", "SubsDens/totalabund"), row.names = FALSE, position="H", caption = "V de Cramer (MNAR)")
```
Le V de Cramer est tres proche de 1 pour chaque paire de variable et chaque dispositif, ce qui signifie que les variables ont une forte relation 2 à 2. 


**Analyse muldimentionnelle** 

Nous utiliserons la fonction `MCA` du package `FactoMineR` pour l'analyse multidimensionnelle.
L’analyse des correspondances multiples est une technique descriptive visant à résumer l’information contenu dans un grand nombre de variables afin de faciliter l’interprétention des corrélations existantes entre ces différentes variables. On cherche à savoir quelles sont les variables corrélées entre elles.
L’analyse se concentre sur ses deux premiers axes qui constitueront un bon résumé des variations observables dans le jeu de données.
Le nom des variable finissant par "-m" signifie les données manquantes pour cette variable, contrairement au nom des variables se termiant par "-o". 

```{r, echo = F, fig.height=4, fig.cap=MCAR}
mis.ind <- matrix("o",nrow=nrow(amp.mcar$amp),ncol=ncol(amp.mcar$amp))
mis.ind[is.na(amp.mcar$amp)] <- "m"
dimnames(mis.ind) <- dimnames(amp.mcar$amp)
resMCA <- MCA(mis.ind)
plot(resMCA,invis="ind",title="MCA graph of the categories")
```

```{r, echo = F, fig.height=4, fig.cap=MAR}
mis.ind <- matrix("o",nrow=nrow(amp.mar$amp),ncol=ncol(amp.mar$amp))
mis.ind[is.na(amp.mar$amp)] <- "m"
dimnames(mis.ind) <- dimnames(amp.mar$amp)
resMCA <- MCA(mis.ind)
plot(resMCA,invis="ind",title="MCA graph of the categories")
```

```{r, echo = F, fig.height=4, fig.cap=MNAR}
mis.ind <- matrix("o",nrow=nrow(amp.mnar$amp),ncol=ncol(amp.mnar$amp))
mis.ind[is.na(amp.mnar$amp)] <- "m"
dimnames(mis.ind) <- dimnames(amp.mnar$amp)
resMCA <- MCA(mis.ind)
plot(resMCA,invis="ind",title="MCA graph of the categories")
```

DONT KNOW
Je sais pas/plus comment lire ces graphes. Quoi en dire ??? -> Rien sur le diapo de Lise
 
### Imputation les données manquantes

Nous allons maintenant imputer les données manquantes sur les 3 dispositifs que nous avons crés précédement.

**Imputation simple** 

#### Par la moyenne
\  

Nous utilisons  la fonction `impute` du package `Hmisc`.

**MCAR**
```{r}
dat.moy.mcar=impute(amp.mcar$amp, what=mean)
```

**MAR**
```{r}
dat.moy.mar=impute(amp.mar$amp, what=mean)
```

**MNAR**
```{r}
dat.moy.mnar=impute(amp.mnar$amp, what=mean)
```

#### Par la médiane
\  

Nous utilisons aussi la fonction `impute` du package `Hmisc`.
**MCAR**
```{r}
dat.med.mcar=impute(amp.mcar$amp, fun=median)
```

**MAR**
```{r}
dat.med.mar=impute(amp.mar$amp, fun=median)
```

**MNAR**
```{r}
dat.med.mnar=impute(amp.mnar$amp, fun=median)
```

#### LOVF (Last observation carries forward)
\  

Cette méthode remplace la valeur manquante par la dernière valeur disponible (cas de données longitudinales). Cependant, cette méthode peut donner des résultats biaisées, même en présence de MCAR. Nous utilisons la fonction `na.locf` du package `zoo`.

**MCAR**
```{r}
dat.locf=na.locf(amp.mcar$amp,na.rm=FALSE)
dat.locf=na.locf(dat.locf,na.rm=FALSE,
   fromLast=TRUE)
```

**MAR**
```{r}
dat.locf=na.locf(amp.mar$amp,na.rm=FALSE)
dat.locf=na.locf(dat.locf,na.rm=FALSE,
   fromLast=TRUE)
```

**MNAR**
```{r}
dat.locf=na.locf(amp.mnar$amp,na.rm=FALSE)
dat.locf=na.locf(dat.locf,na.rm=FALSE,
   fromLast=TRUE)
```

#### k plus proches voisins kNN
\  

L’idée de cette méthode est de calculer les distances entre observations, et d’attribuer aux données manquantes la moyenne des valeurs observées chez les k plus proches voisins. Nous utilisons la fonction `kNN` du package `VIM`.

**MCAR**
```{r}
dat.kNN=kNN(amp.mcar$amp, k=5, imp_var=FALSE)
```

**MAR**
```{r}
dat.kNN=kNN(amp.mar$amp, k=5, imp_var=FALSE)
```

**MNAR**
```{r}
dat.kNN=kNN(amp.mnar$amp, k=5, imp_var=FALSE)
```

#### Loess (Régression locale)
\  

Cette méthode consiste à construire un polynôme de degré faible par méthode des moindres carrés pondérés en fonction de la proximité entre les observations jugées semblables à celle sur laquelle nous voulons procéder à l’imputation. 

**MCAR**
```{r}
dat.imputed=rbind(colnames(amp.mcar$amp),amp.mcar$amp)
indices=1:nrow(amp.mcar$amp)
dat.loess= apply(dat.imputed, 2, function(j) {
predict(locfit(j[-1] ~ indices), indices)
})
head(dat.loess)
```
Avec cette méthode nous remarquons que les variables `Galumna` et `totalabund` ne sont plus des valeurs entières pour les données manquantes imputées. Nous ne prenons donc pas cette méthode en considération dans notre analyse.

#### missForests
\  

**MCAR**
```{r}
dat.missForest<-missForest(amp.mcar$amp,maxiter=10,
        ntree = 200, variablewise = TRUE)$ximp
head(dat.missForest)
```

Comme pour la méthode Loess, nous remarquons que les variables `Galumna` et `totalabund` ne sont plus des valeurs entières pour les données manquantes imputées.

A noter que la méthode SVD, décomposition en valeurs singulières, ne fonctionnent pas dans notre cas. Il y a trop de données manquantes, cela introduit un biais important dans le calcul de base de décomposition.

**Imputation multiple** avec `mice`

\newpage

# Références bibliographiques

<https://delladata.fr/>\
<https://mran.microsoft.com/snapshot/2017-04-24/web/packages/mice/vignettes/ampute.html>\
<https://www.rdocumentation.org/>\
 

---
nocite: |
  @CoursLB
---
